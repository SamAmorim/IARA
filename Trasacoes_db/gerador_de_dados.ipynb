{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a008c2eb-7fac-4784-807c-7119a1a84188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5844a3af-048a-4579-8a64-5e7ef630f800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e316d1ba-aa89-4bb7-9493-a8443b000461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d412762-c888-4670-acc5-40398b683b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parâmetros de Configuração\n",
    "\n",
    "- **NOME_APLICACAO_SPARK**: Nome identificador desta execução no ambiente Spark.\n",
    "- **ANO_ESTATISTICA**: Ano base utilizado para buscar as estatísticas de volume de transações.\n",
    "- **FATOR_ESCALA_VOLUME**: Fator de escala para reduzir o volume total de transações geradas. Por exemplo, `0.006` indica que será gerado apenas 0,6% do volume real, útil para criar amostras menores e mais rápidas de processar.\n",
    "- **LIMITE_ABSOLUTO_TX**: Limite máximo absoluto de transações a serem geradas, evitando volumes excessivos.\n",
    "- **LIMITE_MUNICIPIOS_PROCESSADOS**: Quantidade máxima de municípios para os quais serão gerados dados, priorizando aqueles com maior volume.\n",
    "- **PROBABILIDADE_TRANSACAO_INTERMUNICIPAL**: Proporção de transações que terão como destino um município diferente do de origem (ex: 20%), tornando a simulação mais realista.\n",
    "\n",
    "---\n",
    "\n",
    "### Parâmetros de Fraude\n",
    "\n",
    "- **PROBABILIDADE_FRAUDE_BASE**: Probabilidade padrão de uma transação ser fraudulenta (ex: 5%).\n",
    "- **PROB_CONTA_ALTO_RISCO**: Probabilidade de uma conta ser classificada como \"alto risco\" ao ser criada (ex: 10%).\n",
    "- **PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO**: Probabilidade de fraude quando o destino é uma conta de alto risco (ex: 60%).\n",
    "- **PROBABILIDADE_FRAUDE_CHAVE_RECENTE**: Probabilidade de fraude quando o PIX é destinado a uma chave recém-cadastrada (ex: 40%), já que contas novas são frequentemente usadas em golpes.\n",
    "- **DIAS_CHAVE_CONSIDERADA_RECENTE**: Número de dias para considerar uma chave como \"recente\" (ex: 7 dias).\n",
    "- **MAX_DIAS_CADASTRO_CHAVE_RISCO**: Prazo máximo para contas de alto risco cadastrarem suas chaves PIX após a abertura (ex: 5 dias).\n",
    "\n",
    "---\n",
    "\n",
    "### Parâmetros Gerais\n",
    "\n",
    "- **MULTIPLICADOR_MAGNITUDE_OUTLIER**: Multiplicador para definir o valor de uma transação \"outlier\" (ex: 25 vezes o valor base).\n",
    "- **MULTIPLICADOR_MAGNITUDE_FRAUDE**: Multiplicador para definir o valor de uma transação fraudulenta (ex: 50 vezes o valor base), simulando tentativas de golpes de alto valor.\n",
    "- **PROBABILIDADES_TIPO_FRAUDE**: Dicionário que define a chance de cada tipo de fraude ocorrer. Exemplo: `valor_atipico` tem 30% de chance de ser o tipo escolhido em caso de fraude.\n",
    "- **PESO_CONTAS_POS_PIX**: Proporção de contas criadas após o lançamento do PIX (ex: 70%), refletindo o aumento da bancarização no período."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ad0bfd-3b2c-49a8-b619-aa39da66a56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA 1: IMPORTS E CONFIGURAÇÃO (REFATORADA)\n",
    "# =============================================================================\n",
    "\n",
    "# --- Imports Padrão ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import random\n",
    "import calendar\n",
    "import functools\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Iterator, List\n",
    "from faker import Faker\n",
    "\n",
    "# --- Imports do PySpark ---\n",
    "from pyspark.sql import DataFrame, Window, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType, DateType, TimestampType, BooleanType\n",
    "\n",
    "# --- Inicialização do Spark (Em Databricks, isso é automático) ---\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# =============================================================================\n",
    "# ! ATENÇÃO: CONFIGURAÇÕES, SEED E SCHEMAS\n",
    "# =============================================================================\n",
    "\n",
    "# ! REATORAÇÃO: Seed global para garantir reprodutibilidade\n",
    "GLOBAL_SEED = 42\n",
    "Faker.seed(GLOBAL_SEED) # Seed para a biblioteca Faker\n",
    "print(f\"INFO: Sessão Spark importada. Seed global definida: {GLOBAL_SEED}\")\n",
    "\n",
    "\n",
    "# --- Configurações de Geração (Sem alteração na lógica) ---\n",
    "ANO_ESTATISTICA = 2024\n",
    "LIMITE_MUNICIPIOS_PROCESSADOS = 1\n",
    "FATOR_ESCALA_VOLUME = 0.0005\n",
    "TX_POR_CLIENTE_ESPERADO = 150 \n",
    "PROBABILIDADE_TRANSACAO_INTERMUNICIPAL = 0.20\n",
    "\n",
    "# Dicionários de configuração (sem alteração)\n",
    "config_geracao = {\n",
    "    'PROB_CONTA_ALTO_RISCO': 0.05,\n",
    "    'MAX_DIAS_CADASTRO_CHAVE_RISCO': 7,\n",
    "    'DIAS_CHAVE_CONSIDERADA_RECENTE': 15,\n",
    "    'PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO': 0.60,\n",
    "    'PROBABILIDADE_FRAUDE_CHAVE_RECENTE': 0.40,\n",
    "    'PROBABILIDADE_FRAUDE_BASE': 0.35, \n",
    "    'IDADE_MINIMA_ALVO_ENG_SOCIAL': 55, \n",
    "    'PROBABILIDADE_FRAUDE_ENG_SOCIAL_ALVO': 0.80,\n",
    "    'MULTIPLICADOR_MAGNITUDE_FRAUDE': 30.0,\n",
    "    'PROBABILIDADE_ABAIXO_RADAR': 0.4,\n",
    "    'VALORES_LIMITE_RADAR': [999.90, 499.90, 1999.90],\n",
    "    'PROBABILIDADES_TIPO_FRAUDE': {\n",
    "        \"valor_atipico\": 0.80,\n",
    "        \"engenharia_social\": 0.0,\n",
    "        \"triangulacao_conta_laranja\": 0.10,\n",
    "        \"consolidacao_fundos\": 0.10\n",
    "    },\n",
    "    'MIN_FONTES_CONSOLIDACAO': 10,  \n",
    "    'MAX_FONTES_CONSOLIDACAO': 30,  \n",
    "    'JANELA_SEG_CONSOLIDACAO': 600, \n",
    "    'FANOUT_MIN_PROFUNDIDADE': 2,\n",
    "    'FANOUT_MAX_PROFUNDIDADE': 3,\n",
    "    'FANOUT_MIN_LARGURA': 2,\n",
    "    'FANOUT_MAX_LARGURA': 4,\n",
    "    'PROBABILIDADE_TESTE_CONTA': 0.3,\n",
    "    'PROBABILIDADE_ATAQUE_MADRUGADA': 0.70,\n",
    "}\n",
    "\n",
    "perfis_de_uso_dict = {} \n",
    "LISTA_TIPOS_CONTA_LOCAL = [1, 2, 4] \n",
    "LISTA_ISPBS_LOCAL = [\"12345678\", \"87654321\"] \n",
    "\n",
    "# =============================================================================\n",
    "# --- SCHEMAS (Campos 'id' definidos como nullable=False) ---\n",
    "# =============================================================================\n",
    "SCHEMA_CLIENTES_UDF = StructType([\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"registro_nacional\", StringType(), True),\n",
    "    StructField(\"nascido_em\", DateType(), True)\n",
    "])\n",
    "SCHEMA_CLIENTES_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"id_natureza\", IntegerType(), True), StructField(\"registro_nacional\", StringType(), True),\n",
    "    StructField(\"nascido_em\", DateType(), True), StructField(\"estado_ibge\", IntegerType(), True),\n",
    "    StructField(\"municipio_ibge\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_CONTAS_UDF = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"saldo\", DoubleType(), True),\n",
    "    StructField(\"aberta_em\", DateType(), True), StructField(\"agencia\", StringType(), True),\n",
    "    StructField(\"numero\", StringType(), True), StructField(\"id_tipo_conta\", IntegerType(), True),\n",
    "    StructField(\"ispb_instituicao\", StringType(), True), StructField(\"id_cliente\", StringType(), True),\n",
    "    StructField(\"is_high_risk\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_CONTAS_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"saldo\", DoubleType(), True),\n",
    "    StructField(\"aberta_em\", DateType(), True), StructField(\"agencia\", StringType(), True),\n",
    "    StructField(\"numero\", StringType(), True), StructField(\"id_tipo_conta\", IntegerType(), True),\n",
    "    StructField(\"ispb_instituicao\", StringType(), True), StructField(\"id_cliente\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"is_high_risk\", IntegerType(), True), StructField(\"estado_ibge\", IntegerType(), True),\n",
    "    StructField(\"municipio_ibge\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_CHAVES_UDF = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"chave\", StringType(), True),\n",
    "    StructField(\"id_tipo_chave\", IntegerType(), True), StructField(\"cadastrada_em\", DateType(), True),\n",
    "    StructField(\"id_conta\", StringType(), True)\n",
    "])\n",
    "SCHEMA_CHAVES_PIX_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"chave\", StringType(), True),\n",
    "    StructField(\"id_tipo_chave\", IntegerType(), True), StructField(\"cadastrada_em\", DateType(), True),\n",
    "    StructField(\"id_conta\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"estado_ibge\", IntegerType(), True),\n",
    "    StructField(\"municipio_ibge\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_PARES = StructType([\n",
    "    StructField(\"id_conta_origem\", StringType(), True),\n",
    "    StructField(\"id_conta_destino\", StringType(), True)\n",
    "])\n",
    "SCHEMA_TRANSACOES_UDF = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"data\", TimestampType(), True), StructField(\"mensagem\", StringType(), True),\n",
    "    StructField(\"id_conta_origem\", StringType(), True), StructField(\"id_conta_destino\", StringType(), True),\n",
    "    StructField(\"id_tipo_iniciacao_pix\", IntegerType(), True), StructField(\"id_finalidade_pix\", IntegerType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True), StructField(\"fraud_type\", StringType(), True),\n",
    "    StructField(\"id_transacao_cadeia_pai\", StringType(), True)\n",
    "])\n",
    "SCHEMA_TRANSACOES_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), # ! REATORAÇÃO\n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"data\", TimestampType(), True), StructField(\"mensagem\", StringType(), True),\n",
    "    StructField(\"id_conta_origem\", StringType(), True), StructField(\"id_conta_destino\", StringType(), True),\n",
    "    StructField(\"id_tipo_iniciacao_pix\", IntegerType(), True), StructField(\"id_finalidade_pix\", IntegerType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True), StructField(\"fraud_type\", StringType(), True),\n",
    "    StructField(\"id_transacao_cadeia_pai\", StringType(), True),\n",
    "    StructField(\"estado_ibge\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\"INFO: CÉLULA 1 (Refatorada) - Configurações, Seed e Schemas carregados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c098b61a-0d3b-4791-8acc-95c85081808b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicialização do Ambiente e Esquemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d762d55c-9a11-44a3-9ec2-ff5e2af59c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a5b8147d-718b-41bc-8b76-8743167c9aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE transacoes_db.pix_baseline_metricas.volumes_anuais_por_municipio\n",
    "COMMENT 'Tabela agregada com os volumes totais anuais por município para acelerar o pipeline de geração de dados.'\n",
    "AS\n",
    "SELECT\n",
    "  ano AS Ano,\n",
    "  Municipio_Ibge AS cod_ibge_municipio,\n",
    "  Municipio AS municipio_nome,\n",
    "  \n",
    "  -- ALTERAÇÃO APLICADA AQUI: Derivando o código do estado a partir do código do município\n",
    "  CAST(SUBSTRING(CAST(Municipio_Ibge AS STRING), 1, 2) AS INT) AS cod_ibge_estado,\n",
    "  \n",
    "  -- Calculando o total de transações de pagadores\n",
    "  SUM(total_tx_pf_pagador + total_tx_pj_pagador) AS volume_pagador_anual,\n",
    "  \n",
    "  -- Mantendo os nomes das colunas de origem para o cálculo do número de clientes\n",
    "  SUM(total_tx_pf_pagador) AS total_pf_anual,\n",
    "  SUM(total_tx_pj_pagador) AS total_pj_anual\n",
    "FROM\n",
    "  transacoes_db.pix_baseline_metricas.relacao_pagadores_recebedores\n",
    "GROUP BY\n",
    "  ano,\n",
    "  Municipio_Ibge,\n",
    "  Municipio,\n",
    "  -- Agrupando também pela coluna derivada\n",
    "  CAST(SUBSTRING(CAST(Municipio_Ibge AS STRING), 1, 2) AS INT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe155e97-b861-451c-90f6-c6ebc3a72eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE transacoes_db.pix_baseline_metricas.relacao_pagadores_recebedores;\n",
    "\n",
    "\n",
    "OPTIMIZE transacoes_db.pix_baseline_metricas.relacao_pagadores_recebedores\n",
    "ZORDER BY (ano, Municipio_Ibge);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7511c2-b2cb-4d6b-843c-22ae96a527c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções de Geração e Salvamento\n",
    "\n",
    "As funções abaixo trabalham em conjunto para criar a população e as transações de um município:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `_gerar_clientes`, `_gerar_contas`, `_gerar_chaves_pix`\n",
    "\n",
    "- **_gerar_clientes**  \n",
    "  Cria o número especificado de Pessoas Físicas (PF) e Jurídicas (PJ), gerando IDs, nomes, CPFs/CNPJs e datas de nascimento/fundação.\n",
    "\n",
    "- **_gerar_contas**  \n",
    "  Para cada cliente, cria um número aleatório de contas bancárias.\n",
    "\n",
    "- **_gerar_chaves_pix**  \n",
    "  Para cada conta, gera uma chave PIX (CPF, e-mail, telefone, etc.), garantindo que a data de cadastro da chave seja sempre posterior à data de abertura da conta.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `_gerar_detalhes_transacao_python_vetorizado`\n",
    "\n",
    "Função responsável por criar os detalhes de cada transação:\n",
    "\n",
    "- **Data da Transação:**  \n",
    "  Seleciona uma data e hora aleatória dentro do mês de referência.\n",
    "\n",
    "- **Lógica de Fraude:**  \n",
    "  Define a probabilidade de fraude com base em regras, como se a conta destino é de risco ou se a chave é recente.\n",
    "\n",
    "- **Valor por Tipo de Conta:**  \n",
    "  Gera um valor monetário condizente com o tipo de transação (ex: salário, transferência para poupança, etc.).\n",
    "\n",
    "- **Multiplicadores de Fraude e Outlier:**  \n",
    "  Aumenta drasticamente o valor da transação se ela for fraudulenta ou um outlier.\n",
    "\n",
    "- **Fraudes em Cadeia:**  \n",
    "  Para certos tipos de fraude, divide o valor inicial em várias subtransações menores, simulando lavagem de dinheiro.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `gerar_transacoes`\n",
    "\n",
    "Orquestra a geração das transações para um município em um determinado mês:\n",
    "\n",
    "- **Divisão do Volume:**  \n",
    "  Separa o total de transações em locais e intermunicipais.\n",
    "\n",
    "- **Geração de Pares Locais:**  \n",
    "  Sorteia aleatoriamente pares de contas (origem e destino) do mesmo município.\n",
    "\n",
    "- **Geração de Pares Intermunicipais:**  \n",
    "  Sorteia contas de origem do município atual e contas de destino de outros municípios.\n",
    "\n",
    "- **União e Enriquecimento:**  \n",
    "  Junta os dois conjuntos de pares e busca as informações necessárias para gerar os detalhes de cada transação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31bc5ee-a950-4064-8ec2-d688a095b1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA 2: FUNÇÕES DE GERAÇÃO (REFATORADA V3 - CORREÇÃO DE TIPO DE SEED)\n",
    "# =============================================================================\n",
    "\n",
    "# --- Funções de População (Determinísticas) ---\n",
    "\n",
    "# (UDF _gerar_detalhes_cliente_udf é OK, Faker já foi semeado na Célula 1)\n",
    "@F.pandas_udf(SCHEMA_CLIENTES_UDF)\n",
    "def _gerar_detalhes_cliente_udf(id_natureza: pd.Series) -> pd.DataFrame:\n",
    "    # Faker.seed() já foi chamado na Célula 1, garantindo determinismo aqui\n",
    "    local_fake = Faker('pt_BR')\n",
    "    nomes, registros, nascimentos = [], [], []\n",
    "    for nat in id_natureza:\n",
    "        if nat == 1:\n",
    "            nomes.append(local_fake.name())\n",
    "            registros.append(local_fake.cpf())\n",
    "            nascimentos.append(local_fake.date_of_birth(minimum_age=18, maximum_age=80))\n",
    "        else:\n",
    "            nomes.append(local_fake.company())\n",
    "            registros.append(local_fake.cnpj())\n",
    "            nascimentos.append(local_fake.date_between(start_date='-20y', end_date='-1y'))\n",
    "    return pd.DataFrame({\"nome\": nomes, \"registro_nacional\": registros, \"nascido_em\": nascimentos})\n",
    "\n",
    "def _gerar_clientes(num_pf: int, num_pj: int, estado_ibge: int, municipio_ibge: int) -> DataFrame:\n",
    "    df_base_pf = spark.range(num_pf).withColumn(\"id_natureza\", F.lit(1))\n",
    "    df_base_pj = spark.range(num_pj).withColumn(\"id_natureza\", F.lit(2))\n",
    "    df_base_clientes = df_base_pf.union(df_base_pj)\n",
    "    return (df_base_clientes.withColumn(\"id\", F.expr(\"uuid()\"))\n",
    "        .withColumn(\"detalhes\", _gerar_detalhes_cliente_udf(F.col(\"id_natureza\")))\n",
    "        .select(\"id\", F.col(\"detalhes.nome\").alias(\"nome\"), \"id_natureza\",\n",
    "                F.col(\"detalhes.registro_nacional\").alias(\"registro_nacional\"),\n",
    "                F.col(\"detalhes.nascido_em\").alias(\"nascido_em\"),\n",
    "                F.lit(estado_ibge).alias(\"estado_ibge\"), F.lit(municipio_ibge).alias(\"municipio_ibge\")))\n",
    "\n",
    "# UDF Python que aceita 'seed'\n",
    "def _gerar_detalhes_conta_python(iterator: Iterator[pd.DataFrame], config: dict, tipos_conta: list, ispbs: list, ano_estatistica: int, seed: int) -> Iterator[pd.DataFrame]:\n",
    "    # Garantir determinismo dentro da UDF\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    local_fake = Faker('pt_BR')\n",
    "    Faker.seed(seed) # Semeia a instância local do Faker\n",
    "    \n",
    "    data_limite_abertura = datetime(ano_estatistica, 1, 1).date()\n",
    "    for lote in iterator:\n",
    "        resultados = []\n",
    "        for row in lote.itertuples(index=False):\n",
    "            is_high_risk = 1 if random.random() < config['PROB_CONTA_ALTO_RISCO'] else 0\n",
    "            if is_high_risk == 1:\n",
    "                start_date_relativa = timedelta(days=180)\n",
    "                data_inicio_recente = data_limite_abertura - start_date_relativa\n",
    "                aberta_em = local_fake.date_between(start_date=data_inicio_recente, end_date=data_limite_abertura)\n",
    "            else:\n",
    "                aberta_em = local_fake.date_between(start_date='-10y', end_date=data_limite_abertura)\n",
    "            if row.id_natureza == 1:\n",
    "                saldo = round(np.random.lognormal(mean=6, sigma=1.5), 2)\n",
    "                tipo_conta = random.choice(tipos_conta)\n",
    "            else:\n",
    "                saldo = round(np.random.lognormal(mean=9, sigma=1.8), 2)\n",
    "                tipo_conta = random.choice([c for c in tipos_conta if c in [1, 3]])\n",
    "            resultados.append({\"id\": str(uuid.uuid4()), \"saldo\": saldo, \"aberta_em\": aberta_em, \"agencia\": local_fake.numerify('####'), \n",
    "                               \"numero\": local_fake.numerify('#####-#'), \"id_tipo_conta\": tipo_conta, \"ispb_instituicao\": random.choice(ispbs),\n",
    "                               \"id_cliente\": row.id_cliente, \"is_high_risk\": is_high_risk})\n",
    "        yield pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_contas(df_clientes: DataFrame, estado_ibge: int, municipio_ibge: int, seed: int) -> DataFrame:\n",
    "    # Adicionada seed ao F.rand()\n",
    "    df_clientes_com_num_contas = df_clientes.withColumn(\"num_contas\", \n",
    "        F.when(F.col(\"id_natureza\") == 1, F.floor(F.rand(seed=seed) * 2) + 1)\n",
    "         .otherwise(F.floor(F.rand(seed=seed + 1) * 5) + 1))\n",
    "         \n",
    "    df_contas_base = df_clientes_com_num_contas.select(F.col(\"id\").alias(\"id_cliente\"), \"id_natureza\", F.explode(F.sequence(F.lit(1), F.col(\"num_contas\"))))\n",
    "    \n",
    "    # Passando a seed para a UDF\n",
    "    gerador_com_contexto = functools.partial(_gerar_detalhes_conta_python, \n",
    "                                             config=config_geracao, \n",
    "                                             tipos_conta=LISTA_TIPOS_CONTA_LOCAL, \n",
    "                                             ispbs=LISTA_ISPBS_LOCAL, \n",
    "                                             ano_estatistica=ANO_ESTATISTICA,\n",
    "                                             seed=seed) # <-- Seed injetada\n",
    "                                             \n",
    "    return (df_contas_base.mapInPandas(gerador_com_contexto, schema=SCHEMA_CONTAS_UDF)\n",
    "            .withColumn(\"estado_ibge\", F.lit(estado_ibge)).withColumn(\"municipio_ibge\", F.lit(municipio_ibge)))\n",
    "\n",
    "# UDF Python que aceita 'seed'\n",
    "def _gerar_detalhes_chave_udf(iterator: Iterator[pd.DataFrame], config: dict, seed: int) -> Iterator[pd.DataFrame]:\n",
    "    # Garantir determinismo dentro da UDF\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    local_fake = Faker('pt_BR')\n",
    "    Faker.seed(seed) # Semeia a instância local do Faker\n",
    "\n",
    "    for lote in iterator:\n",
    "        resultados = []\n",
    "        for row in lote.itertuples(index=False):\n",
    "            try:\n",
    "                data_abertura_obj = pd.to_datetime(row.aberta_em).date()\n",
    "                dias_para_cadastrar = random.randint(1, config['MAX_DIAS_CADASTRO_CHAVE_RISCO']) if hasattr(row, 'is_high_risk') and row.is_high_risk == 1 else random.randint(1, 90)\n",
    "                cadastrada_em = data_abertura_obj + timedelta(days=dias_para_cadastrar)\n",
    "                if row.id_natureza == 1:\n",
    "                    tipos_possiveis = {1: row.registro_nacional, 2: local_fake.email(), 3: local_fake.phone_number(), 4: str(uuid.uuid4())}\n",
    "                else:\n",
    "                    tipos_possiveis = {5: row.registro_nacional, 2: local_fake.company_email(), 4: str(uuid.uuid4())}\n",
    "                tipo_chave = random.choice(list(tipos_possiveis.keys()))\n",
    "                resultados.append({\"id\": str(uuid.uuid4()), \"chave\": tipos_possiveis[tipo_chave], \"id_tipo_chave\": tipo_chave, \"cadastrada_em\": cadastrada_em, \"id_conta\": row.id_conta})\n",
    "            except Exception as e: \n",
    "                sys.stderr.write(f\"ERRO NA GERAÇÃO DE CHAVES: {e}, DADOS: {row}\\n\")\n",
    "                raise e\n",
    "        if resultados: yield pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_chaves_pix(df_contas_completo: DataFrame, df_clientes_completo: DataFrame, estado_ibge: int, municipio_ibge: int, seed: int) -> DataFrame:\n",
    "    df_contas_com_cliente = df_contas_completo.join(df_clientes_completo, df_contas_completo.id_cliente == df_clientes_completo.id, \"inner\").select(df_contas_completo.id.alias(\"id_conta\"), \"id_natureza\", \"registro_nacional\", \"aberta_em\", df_contas_completo.is_high_risk)\n",
    "    \n",
    "    # Passando a seed para a UDF\n",
    "    gerador_com_contexto = functools.partial(_gerar_detalhes_chave_udf, \n",
    "                                             config=config_geracao,\n",
    "                                             seed=seed) # <-- Seed injetada\n",
    "                                             \n",
    "    return (df_contas_com_cliente.mapInPandas(gerador_com_contexto, schema=SCHEMA_CHAVES_UDF)\n",
    "            .withColumn(\"estado_ibge\", F.lit(estado_ibge)).withColumn(\"municipio_ibge\", F.lit(municipio_ibge)))\n",
    "\n",
    "# --- Funções de Geração de Transações (Determinísticas) ---\n",
    "def _obter_params_tempo(ano: int, mes: int) -> tuple:\n",
    "    num_dias = calendar.monthrange(ano, mes)[1]\n",
    "    primeiro_dia = datetime(ano, mes, 1)\n",
    "    ultimo_dia = datetime(ano, mes, num_dias, 23, 59, 59)\n",
    "    return primeiro_dia, (ultimo_dia - primeiro_dia).total_seconds()\n",
    "\n",
    "# Função interna agora usa 'random_instance'\n",
    "def _aplicar_horario_suspeito(data_transacao, config, random_instance):\n",
    "    if random_instance.random() < config.get('PROBABILIDADE_ATAQUE_MADRUGADA', 0.70):\n",
    "        return data_transacao.replace(hour=random_instance.randint(1, 4), minute=random_instance.randint(0, 59))\n",
    "    return data_transacao\n",
    "\n",
    "def _obter_perfil_sazonal_mes(ano: int, mes: int) -> dict:\n",
    "    # (Lógica original mantida, pois é determinística)\n",
    "    pesos_dia_semana = { 0: 1.2, 1: 1.1, 2: 1.1, 3: 1.1, 4: 1.3, 5: 0.8, 6: 0.7 }\n",
    "    pesos_comemorativos = { (1, 1): 0.5, (5, 10): 2.0, (6, 12): 1.8, (8, 11): 1.8, (10, 12): 1.5, (11, 25): 3.0, (12, 20): 2.0, (12, 23): 2.5, (12, 24): 3.0, (12, 25): 1.0, (12, 31): 1.5 }\n",
    "    num_dias = calendar.monthrange(ano, mes)[1]\n",
    "    perfil_final = {}\n",
    "    for dia in range(1, num_dias + 1):\n",
    "        data = datetime(ano, mes, dia)\n",
    "        dia_semana = data.weekday()\n",
    "        peso = pesos_dia_semana.get(dia_semana, 1.0)\n",
    "        if (mes, dia) in pesos_comemorativos: peso = pesos_comemorativos[(mes, dia)]\n",
    "        perfil_final[dia] = peso\n",
    "    return perfil_final\n",
    "\n",
    "# =============================================================================\n",
    "# ! INÍCIO DA MODIFICAÇÃO V3: _gerar_detalhes_transacao_python_vetorizado\n",
    "# =============================================================================\n",
    "# UDF agora usa 'pid' (partition_id) e CONVERTE PARA INT\n",
    "def _gerar_detalhes_transacao_python_vetorizado(\n",
    "    iterator: Iterator[pd.DataFrame], ano: int, mes: int, \n",
    "    config: dict, perfis_uso: list, \n",
    "    perfil_sazonal: dict, seed: int # 'seed' aqui é a 'base_seed'\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "    # Imports necessários dentro da UDF\n",
    "    import random\n",
    "    import uuid\n",
    "    from datetime import timedelta\n",
    "    from itertools import chain\n",
    "    \n",
    "    # --- Inicialização das Variáveis (Fora do Loop) ---\n",
    "    primeiro_dia, _ = _obter_params_tempo(ano, mes) \n",
    "    colunas_finais_transacoes = [\"id\", \"valor\", \"data\", \"mensagem\", \"id_conta_origem\", \"id_conta_destino\", \"id_tipo_iniciacao_pix\", \"id_finalidade_pix\", \"is_fraud\", \"fraud_type\", \"id_transacao_cadeia_pai\"]\n",
    "    PROB_RUIDO = 0.25; MENSAGENS_RUIDO = [\"Pagamento de Boleto\", \"Lanchonete\", \"Uber\", \"Recarga de Celular\"]\n",
    "    \n",
    "    # Instâncias de aleatoriedade são inicializadas UMA VEZ por partição (iterator)\n",
    "    try:\n",
    "        primeiro_lote = next(iterator)\n",
    "    except StopIteration:\n",
    "        # Partição vazia, não faz nada\n",
    "        return\n",
    "        \n",
    "    # Pega o 'pid' do primeiro lote (será o mesmo para toda a partição)\n",
    "    # ! ===================================================================\n",
    "    # ! CORREÇÃO V3: Converte o partition_id (que é numpy.int) para um \n",
    "    # ! int nativo do Python, que é aceito pela biblioteca 'random'.\n",
    "    # ! ===================================================================\n",
    "    partition_id = int(primeiro_lote['pid'].iloc[0])\n",
    "    lote_seed = int(seed) + partition_id # Cria a seed única e determinística da partição\n",
    "    \n",
    "    # Semeia as instâncias locais\n",
    "    local_random = random.Random(lote_seed)\n",
    "    local_np_random = np.random.RandomState(lote_seed)\n",
    "    \n",
    "    # --- Início do Processamento (Loop Principal) ---\n",
    "    # Nós \"re-adicionamos\" o primeiro lote ao fluxo usando chain\n",
    "    \n",
    "    for lote in chain([primeiro_lote], iterator):\n",
    "        n = len(lote)\n",
    "        if n == 0: continue\n",
    "        \n",
    "        # =========================================================================\n",
    "        # --- LÓGICA: POOL DE CONTAS INTERNO (IN-BATCH) ---\n",
    "        # =========================================================================\n",
    "        pool_de_contas_reais = pd.concat([\n",
    "            lote['id_conta_origem'], \n",
    "            lote['id_conta_destino']\n",
    "        ]).unique().tolist()\n",
    "        \n",
    "        if not pool_de_contas_reais:\n",
    "            pool_de_contas_reais = [str(uuid.uuid4()) for _ in range(10)]\n",
    "        \n",
    "        # --- LÓGICA DE TIMESTAMP SAZONAL (Determinístico) ---\n",
    "        num_dias = calendar.monthrange(ano, mes)[1]; dias_do_mes = list(range(1, num_dias + 1))\n",
    "        pesos = [perfil_sazonal.get(dia, 1.0) for dia in dias_do_mes]\n",
    "        probabilidades = np.array(pesos) / np.sum(pesos)\n",
    "        \n",
    "        # Usa o gerador semeado da partição (local_np_random)\n",
    "        dias_sorteados = local_np_random.choice(dias_do_mes, size=n, p=probabilidades)\n",
    "        segundos_no_dia = local_np_random.uniform(0, 86399, n)\n",
    "        \n",
    "        deltas_dias = pd.to_timedelta(dias_sorteados - 1, unit='D')\n",
    "        deltas_segundos = pd.to_timedelta(segundos_no_dia, unit='s')\n",
    "        lote['data'] = primeiro_dia + deltas_dias + deltas_segundos\n",
    "        \n",
    "        # ==========================================================\n",
    "        # <<< INÍCIO: LÓGICA DE ENG. SOCIAL BASEADA EM IDADE >>>\n",
    "        # ==========================================================\n",
    "        lote['pagador_nascido_em'] = pd.to_datetime(lote['pagador_nascido_em'], errors='coerce')\n",
    "        lote['pagador_idade'] = (lote['data'] - lote['pagador_nascido_em']).dt.days / 365.25\n",
    "        lote['pagador_idade'] = lote['pagador_idade'].fillna(-1) \n",
    "        lote['chave_destino_cadastrada_em'] = pd.to_datetime(lote['chave_destino_cadastrada_em'])\n",
    "        delta_dias = (lote['data'].dt.date - lote['chave_destino_cadastrada_em'].dt.date).dt.days \n",
    "        lote['is_high_risk'] = lote['is_high_risk'].fillna(0).astype(int)\n",
    "        \n",
    "        idade_alvo = config.get('IDADE_MINIMA_ALVO_ENG_SOCIAL', 65)\n",
    "        prob_fraude_alvo = config.get('PROBABILIDADE_FRAUDE_ENG_SOCIAL_ALVO', 0.75)\n",
    "        \n",
    "        cond_idade_alvo = (lote['pagador_idade'] >= idade_alvo)\n",
    "        cond_conta_risco = (lote['is_high_risk'] == 1)\n",
    "        cond_chave_recente = ((delta_dias >= 0) & (delta_dias <= config['DIAS_CHAVE_CONSIDERADA_RECENTE']))\n",
    "        \n",
    "        prob_fraude_dinamica = np.select(\n",
    "            [cond_idade_alvo, cond_conta_risco, cond_chave_recente], \n",
    "            [prob_fraude_alvo, config['PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO'], config['PROBABILIDADE_FRAUDE_CHAVE_RECENTE']], \n",
    "            default=config['PROBABILIDADE_FRAUDE_BASE']\n",
    "        )\n",
    "        \n",
    "        # Usa o gerador semeado da partição\n",
    "        lote['is_fraud'] = (local_np_random.rand(n) < prob_fraude_dinamica).astype(int)\n",
    "        \n",
    "        # --- LÓGICA DE VALOR (Determinístico) ---\n",
    "        multiplicadores = np.select([lote['is_fraud'] == 1, (~(lote['is_fraud'] == 1)) & (local_np_random.rand(n) < 0.04)], [config['MULTIPLICADOR_MAGNITUDE_FRAUDE'], 2.5], default=1.0)\n",
    "        valores_calculados = np.maximum(0.01, local_np_random.lognormal(mean=np.log(150), sigma=0.8, size=n) * multiplicadores).round(2)\n",
    "        condicao_abaixo_radar = (lote['is_fraud'] == 1) & (local_np_random.rand(n) < config.get('PROBABILIDADE_ABAIXO_RADAR', 0.4))\n",
    "        \n",
    "        # Usa o gerador semeado da partição\n",
    "        lote['valor'] = np.where(condicao_abaixo_radar, local_np_random.choice(config.get('VALORES_LIMITE_RADAR', [999.90]), n), valores_calculados)\n",
    "        \n",
    "        # --- LÓGICA DE TIPO DE FRAUDE (Determinístico) ---\n",
    "        probs_tipo_fraude = config.get('PROBABILIDADES_TIPO_FRAUDE'); \n",
    "        tipos_normais = {k: v for k, v in probs_tipo_fraude.items() if k != 'engenharia_social'}\n",
    "        if sum(tipos_normais.values()) == 0: tipos_normais = {'valor_atipico': 1.0}\n",
    "        total_prob_normal = sum(tipos_normais.values())\n",
    "        tipos_normais_norm = {k: v / total_prob_normal for k, v in tipos_normais.items()}\n",
    "        \n",
    "        # Usa o gerador semeado da partição\n",
    "        tipos_fraude_aleatorios = local_np_random.choice(list(tipos_normais_norm.keys()), n, p=list(tipos_normais_norm.values()))\n",
    "        \n",
    "        lote['fraud_type'] = np.where(\n",
    "            (lote['is_fraud'] == 1) & (cond_idade_alvo), \n",
    "            \"engenharia_social\", \n",
    "            np.where(lote['is_fraud'] == 1, tipos_fraude_aleatorios, None) \n",
    "        )\n",
    "        \n",
    "        # --- PREPARAÇÃO FINAL DO LOTE (Determinístico) ---\n",
    "        lote['id'] = [str(uuid.uuid4()) for _ in range(n)]; lote['mensagem'] = \"Pagamento via Pix\"; \n",
    "        lote['id_tipo_iniciacao_pix'] = local_np_random.randint(1, 4, n); \n",
    "        lote['id_finalidade_pix'] = local_np_random.randint(1, 5, n); \n",
    "        lote['id_transacao_cadeia_pai'] = None\n",
    "        \n",
    "        # Remove a coluna 'pid' antes de finalizar\n",
    "        colunas_para_dropar = ['chave_destino_cadastrada_em', 'is_high_risk', 'id_tipo_conta_origem', 'id_tipo_conta_destino', 'pagador_nascido_em', 'pagador_idade', 'pid']\n",
    "        lote_final = lote.drop(columns=[col for col in colunas_para_dropar if col in lote.columns])\n",
    "        \n",
    "        # --- LÓGICA DE MICRO-LOTE ---\n",
    "        resultados_micro_lote = []\n",
    "        TAMANHO_MICRO_LOTE = 2000\n",
    "        \n",
    "        # --- LÓGICA DE GERAÇÃO DE CADEIA (FAN-OUT E FAN-IN) ---\n",
    "        # Usa o gerador semeado da partição (local_random)\n",
    "        for row in lote_final.itertuples(index=False):\n",
    "            \n",
    "            # --- LÓGICA DE FAN-OUT (Dispersão) ---\n",
    "            if hasattr(row, 'is_fraud') and row.is_fraud and row.fraud_type == \"triangulacao_conta_laranja\":\n",
    "                if local_random.random() < config.get('PROBABILIDADE_TESTE_CONTA', 0.3):\n",
    "                    resultados_micro_lote.append({\"id\": str(uuid.uuid4()), \"valor\": round(local_random.uniform(0.01, 1.00), 2), \"data\": row.data - timedelta(minutes=local_random.randint(1, 5)), \"mensagem\": \"Teste\", \"id_conta_origem\": row.id_conta_origem, \"id_conta_destino\": row.id_conta_destino, \"id_tipo_iniciacao_pix\": 1, \"id_finalidade_pix\": 1, \"is_fraud\": 0, \"fraud_type\": None, \"id_transacao_cadeia_pai\": None})\n",
    "                \n",
    "                row_dict = row._asdict(); \n",
    "                row_dict['data'] = _aplicar_horario_suspeito(row_dict['data'], config, local_random) # <-- Instância passada\n",
    "                \n",
    "                min_prof = config.get('FANOUT_MIN_PROFUNDIDADE', 2); max_prof = config.get('FANOUT_MAX_PROFUNDIDADE', 4)\n",
    "                profundidade_alvo = local_random.randint(min_prof, max_prof)\n",
    "                id_fraude_raiz = row_dict['id']; resultados_micro_lote.append(row_dict)\n",
    "                contas_nivel_anterior = {row_dict['id_conta_destino']: row_dict['valor']}; id_pai_nivel_anterior = {row_dict['id_conta_destino']: id_fraude_raiz}; data_nivel_anterior = {row_dict['id_conta_destino']: row_dict['data']}\n",
    "                \n",
    "                for nivel_atual in range(2, profundidade_alvo + 1):\n",
    "                    contas_proximo_nivel = {}; id_pai_proximo_nivel = {}; data_proximo_nivel = {}\n",
    "                    if not contas_nivel_anterior: break\n",
    "                    for conta_origem, valor_origem in contas_nivel_anterior.items():\n",
    "                        min_larg = config.get('FANOUT_MIN_LARGURA', 2); max_larg = config.get('FANOUT_MAX_LARGURA', 5)\n",
    "                        num_subs = local_random.randint(min_larg, max_larg)\n",
    "                        \n",
    "                        # Usa o gerador semeado da partição\n",
    "                        valores_divididos = local_np_random.dirichlet(np.ones(num_subs)) * valor_origem\n",
    "                        \n",
    "                        for k in range(num_subs):\n",
    "                            id_transacao_filha = str(uuid.uuid4())\n",
    "                            id_conta_destino_filha = local_random.choice(pool_de_contas_reais) \n",
    "                            segundos_offset = local_random.uniform(60 * (nivel_atual-1), 3600 * (nivel_atual-1))\n",
    "                            data_transacao_filha = data_nivel_anterior[conta_origem] + timedelta(seconds=segundos_offset); \n",
    "                            data_transacao_filha = _aplicar_horario_suspeito(data_transacao_filha, config, local_random) # <-- Instância passada\n",
    "                            \n",
    "                            transacao_filha = {\"id\": id_transacao_filha, \"valor\": round(max(0.01, valores_divididos[k]), 2), \"data\": data_transacao_filha, \"mensagem\": f\"Dispersão N{nivel_atual} (Parte {k+1}/{num_subs})\", \"id_conta_origem\": conta_origem, \"id_conta_destino\": id_conta_destino_filha, \"id_tipo_iniciacao_pix\": local_random.randint(1, 3), \"id_finalidade_pix\": local_random.randint(1, 4), \"is_fraud\": 1, \"fraud_type\": row.fraud_type, \"id_transacao_cadeia_pai\": id_pai_nivel_anterior[conta_origem]}\n",
    "                            resultados_micro_lote.append(transacao_filha)\n",
    "                            contas_proximo_nivel[id_conta_destino_filha] = transacao_filha[\"valor\"]; id_pai_proximo_nivel[id_conta_destino_filha] = id_transacao_filha; data_proximo_nivel[id_conta_destino_filha] = data_transacao_filha\n",
    "                            \n",
    "                            if local_random.random() < PROB_RUIDO:\n",
    "                                for _ in range(local_random.randint(1, 3)):\n",
    "                                    segundos_offset_ruido = local_random.uniform(10, 3600)\n",
    "                                    id_conta_destino_ruido = local_random.choice(pool_de_contas_reais) \n",
    "                                    resultados_micro_lote.append({\"id\": str(uuid.uuid4()), \"valor\": round(local_random.uniform(7.5, 75.0), 2), \"data\": data_transacao_filha + timedelta(seconds=segundos_offset_ruido), \"mensagem\": random.choice(MENSAGENS_RUIDO), \"id_conta_origem\": id_conta_destino_filha, \"id_conta_destino\": id_conta_destino_ruido, \"id_tipo_iniciacao_pix\": 1, \"id_finalidade_pix\": 1, \"is_fraud\": 0, \"fraud_type\": None, \"id_transacao_cadeia_pai\": None})\n",
    "                    contas_nivel_anterior = contas_proximo_nivel; id_pai_nivel_anterior = id_pai_proximo_nivel; data_nivel_anterior = data_proximo_nivel\n",
    "            \n",
    "            # --- LÓGICA DE FAN-IN (Consolidação) ---\n",
    "            elif hasattr(row, 'is_fraud') and row.is_fraud and row.fraud_type == \"consolidacao_fundos\":\n",
    "                row_dict = row._asdict()\n",
    "                row_dict['data'] = _aplicar_horario_suspeito(row_dict['data'], config, local_random) # <-- Instância passada\n",
    "                resultados_micro_lote.append(row_dict)\n",
    "                id_fraude_raiz = row_dict['id']; data_fraude_raiz = row_dict['data']\n",
    "                id_conta_destino_final = row_dict['id_conta_destino'] \n",
    "                \n",
    "                min_fontes = config.get('MIN_FONTES_CONSOLIDACAO', 3); max_fontes = config.get('MAX_FONTES_CONSOLIDACAO', 10); janela_seg = config.get('JANELA_SEG_CONSOLIDACAO', 3600)\n",
    "                num_fontes_extras = local_random.randint(min_fontes, max_fontes)\n",
    "                \n",
    "                for k in range(num_fontes_extras):\n",
    "                    id_transacao_filha = str(uuid.uuid4())\n",
    "                    id_conta_origem_filha = local_random.choice(pool_de_contas_reais) \n",
    "                    segundos_offset = local_random.uniform(1, janela_seg)\n",
    "                    data_transacao_filha = data_fraude_raiz + timedelta(seconds=segundos_offset)\n",
    "                    data_transacao_filha = _aplicar_horario_suspeito(data_transacao_filha, config, local_random) # <-- Instância passada\n",
    "                    \n",
    "                    # Usa o gerador semeado da partição\n",
    "                    valor_filha = np.maximum(0.01, local_np_random.lognormal(mean=np.log(150), sigma=0.8) * config.get('MULTIPLICADOR_MAGNITUDE_FRAUDE', 30)).round(2)\n",
    "                    \n",
    "                    transacao_filha = {\"id\": id_transacao_filha, \"valor\": valor_filha, \"data\": data_transacao_filha, \"mensagem\": f\"Consolidação Nível 1 (Fonte {k+1}/{num_fontes_extras})\", \"id_conta_origem\": id_conta_origem_filha, \"id_conta_destino\": id_conta_destino_final, \"id_tipo_iniciacao_pix\": local_random.randint(1, 3), \"id_finalidade_pix\": local_random.randint(1, 4), \"is_fraud\": 1, \"fraud_type\": row.fraud_type, \"id_transacao_cadeia_pai\": id_fraude_raiz}\n",
    "                    resultados_micro_lote.append(transacao_filha)\n",
    "            \n",
    "            else:\n",
    "                resultados_micro_lote.append(row._asdict())\n",
    "            \n",
    "            if len(resultados_micro_lote) >= TAMANHO_MICRO_LOTE:\n",
    "                df_final = pd.DataFrame(resultados_micro_lote)\n",
    "                yield df_final[colunas_finais_transacoes]\n",
    "                resultados_micro_lote = []\n",
    "                \n",
    "    if resultados_micro_lote: \n",
    "        df_final = pd.DataFrame(resultados_micro_lote)\n",
    "        yield df_final[colunas_finais_transacoes]\n",
    "\n",
    "# =============================================================================\n",
    "# ! FIM DA MODIFICAÇÃO V3\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ! INÍCIO DA MODIFICAÇÃO V3: gerar_transacoes (Sem mudanças, mas incluída)\n",
    "# =============================================================================\n",
    "def gerar_transacoes(\n",
    "    df_contas_local: DataFrame, \n",
    "    df_clientes_local: DataFrame, \n",
    "    df_chaves_recentes_local: DataFrame, num_contas_local: int,\n",
    "    volume_total: int, estado_ibge: int, municipio_ibge: int, ano: int, mes: int, \n",
    "    municipios_processados: list,\n",
    "    perfil_sazonal: dict,\n",
    "    seed: int # Esta é a 'base_seed' (ex: GLOBAL_SEED + i + mes)\n",
    ") -> DataFrame:\n",
    "    \n",
    "    # Garantir determinismo na escolha do município\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # --- LÓGICA DE GERAÇÃO DE PARES (Determinística e Otimizada) ---\n",
    "    volume_intermunicipal = int(volume_total * PROBABILIDADE_TRANSACAO_INTERMUNICIPAL)\n",
    "    volume_local = volume_total - volume_intermunicipal\n",
    "    df_pares_locais = spark.createDataFrame([], SCHEMA_PARES)\n",
    "    df_pares_intermunicipais = spark.createDataFrame([], SCHEMA_PARES)\n",
    "    \n",
    "    if volume_local > 0 and num_contas_local > 1:\n",
    "        N_BUCKETS = max(100, int(num_contas_local / 10000)) \n",
    "        \n",
    "        # Adicionada seed ao F.rand()\n",
    "        df_origens = df_contas_local.select(\n",
    "            F.col(\"id\").alias(\"id_conta_origem\"), \n",
    "            (F.rand(seed=seed) * N_BUCKETS).cast(\"int\").alias(\"join_key\")\n",
    "        )\n",
    "        df_destinos = df_contas_local.select(\n",
    "            F.col(\"id\").alias(\"id_conta_destino\"), \n",
    "            (F.rand(seed=seed + 1) * N_BUCKETS).cast(\"int\").alias(\"join_key\") # Seed diferente\n",
    "        )\n",
    "        df_pares_locais = df_origens.join(df_destinos, \"join_key\") \\\n",
    "                                  .filter(F.col(\"id_conta_origem\") != F.col(\"id_conta_destino\")) \\\n",
    "                                  .select(\"id_conta_origem\", \"id_conta_destino\") \\\n",
    "                                  .limit(volume_local) # Limit após o join é OK aqui\n",
    "    \n",
    "    outros_municipios = [m for m in municipios_processados if m != municipio_ibge]\n",
    "    \n",
    "    if volume_intermunicipal > 0 and outros_municipios:\n",
    "        municipio_alvo = random.choice(outros_municipios) # Determinístico (random.seed() acima)\n",
    "        \n",
    "        # Substituído orderBy(rand()) por sample()\n",
    "        fraction_origem = min(1.0, (volume_intermunicipal * 1.2) / max(1, num_contas_local))\n",
    "        \n",
    "        contas_origem = df_contas_local.select(\"id\") \\\n",
    "            .sample(withReplacement=False, fraction=fraction_origem, seed=seed) \\\n",
    "            .limit(volume_intermunicipal) \\\n",
    "            .withColumnRenamed(\"id\", \"id_conta_origem\") \\\n",
    "            .withColumn(\"join_key\", F.monotonically_increasing_id())\n",
    "\n",
    "        contas_destino_externas = spark.table(\"transacoes_db.copper.contas\") \\\n",
    "            .filter(F.col(\"municipio_ibge\") == municipio_alvo) \\\n",
    "            .select(\"id\") \\\n",
    "            .sample(withReplacement=False, fraction=fraction_origem, seed=seed + 1) \\\n",
    "            .limit(volume_intermunicipal) \\\n",
    "            .withColumnRenamed(\"id\", \"id_conta_destino\") \\\n",
    "            .withColumn(\"join_key\", F.monotonically_increasing_id())\n",
    "            \n",
    "        df_pares_intermunicipais = contas_origem.join(contas_destino_externas, \"join_key\") \\\n",
    "                                              .select(\"id_conta_origem\", \"id_conta_destino\")\n",
    "    \n",
    "    df_pares_total = df_pares_locais.union(df_pares_intermunicipais)\n",
    "    if df_pares_total.isEmpty(): return spark.createDataFrame([], SCHEMA_TRANSACOES_FINAL)\n",
    "    \n",
    "    # --- LÓGICA DE INNER JOIN (Mantida, está correta) ---\n",
    "    df_pares_enriquecidos = (df_pares_total\n",
    "        .join(df_contas_local.alias(\"orig\"), df_pares_total.id_conta_origem == F.col(\"orig.id\"), \"inner\")\n",
    "        .join(df_clientes_local.alias(\"cli_orig\"), F.col(\"orig.id_cliente\") == F.col(\"cli_orig.id\"), \"inner\")\n",
    "        .join(df_contas_local.alias(\"dest\"), df_pares_total.id_conta_destino == F.col(\"dest.id\"), \"inner\")\n",
    "        .join(df_chaves_recentes_local.alias(\"chaves\"), df_pares_total.id_conta_destino == F.col(\"chaves.id_conta\"), \"left\")\n",
    "        .select(\"id_conta_origem\", \"id_conta_destino\", F.col(\"orig.id_tipo_conta\").alias(\"id_tipo_conta_origem\"),\n",
    "                F.col(\"dest.id_tipo_conta\").alias(\"id_tipo_conta_destino\"), F.col(\"chaves.chave_destino_cadastrada_em\"),\n",
    "                F.col(\"dest.is_high_risk\"),\n",
    "                F.col(\"cli_orig.nascido_em\").alias(\"pagador_nascido_em\")\n",
    "               ))\n",
    "\n",
    "    # Adicionando 'pid' (spark_partition_id) para a UDF usar\n",
    "    df_pares_com_pid = df_pares_enriquecidos.withColumn(\"pid\", F.spark_partition_id())\n",
    "    \n",
    "    # --- PARTIAL (Injetando a 'base_seed') ---\n",
    "    gerador_com_contexto = functools.partial(\n",
    "        _gerar_detalhes_transacao_python_vetorizado, \n",
    "        ano=ano, mes=mes, config=config_geracao, \n",
    "        perfis_uso=perfis_de_uso_dict,\n",
    "        perfil_sazonal=perfil_sazonal,\n",
    "        seed=seed # <-- Esta é a 'base_seed', a UDF irá somar o 'pid' a ela\n",
    "    )\n",
    "    \n",
    "    df_transacoes_bruto = df_pares_com_pid.mapInPandas(gerador_com_contexto, schema=SCHEMA_TRANSACOES_UDF)\n",
    "    return df_transacoes_bruto.withColumn(\"estado_ibge\", F.lit(estado_ibge))\n",
    "\n",
    "# =============================================================================\n",
    "# ! FIM DA MODIFICAÇÃO V3: gerar_transacoes\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# --- Funções Utilitárias e de Orquestração (Com Seed) ---\n",
    "def salvar_dataframe_em_delta(df: DataFrame, nome_tabela_completo: str, modo: str = \"append\"):\n",
    "    if df is None or df.isEmpty():\n",
    "        print(f\"AVISO: DataFrame para a tabela '{nome_tabela_completo}' está vazio. Nenhuma ação tomada.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"INFO: Salvando dados na tabela Delta: {nome_tabela_completo} (modo: {modo})...\")\n",
    "        df.write.format(\"delta\").mode(modo).saveAsTable(nome_tabela_completo)\n",
    "        print(f\"✅ SUCESSO: Dados salvos em {nome_tabela_completo}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERRO ao salvar '{nome_tabela_completo}': {e}\")\n",
    "        raise e\n",
    "\n",
    "# Funções agora aceitam e passam a 'seed'\n",
    "def gerar_e_salvar_populacao(num_pf: int, num_pj: int, estado_ibge: int, municipio_ibge: int, seed: int) -> None:\n",
    "    print(f\"INFO: Gerando e materializando população para {municipio_ibge} (PF: {num_pf}, PJ: {num_pj})...\")\n",
    "    \n",
    "    # (Não precisa de seed, _gerar_clientes usa UDF semeada na Célula 1)\n",
    "    df_clientes_gerado = _gerar_clientes(num_pf, num_pj, estado_ibge, municipio_ibge)\n",
    "    salvar_dataframe_em_delta(df_clientes_gerado, \"transacoes_db.copper.clientes\", modo=\"append\")\n",
    "    \n",
    "    # Recarregar para garantir consistência\n",
    "    df_clientes_materializado = spark.table(\"transacoes_db.copper.clientes\").filter(F.col(\"municipio_ibge\") == municipio_ibge)\n",
    "    \n",
    "    # Passando a seed\n",
    "    df_contas_gerado = _gerar_contas(df_clientes_materializado, estado_ibge, municipio_ibge, seed=seed)\n",
    "    salvar_dataframe_em_delta(df_contas_gerado, \"transacoes_db.copper.contas\", modo=\"append\")\n",
    "    \n",
    "    df_contas_materializado = spark.table(\"transacoes_db.copper.contas\").filter(F.col(\"municipio_ibge\") == municipio_ibge)\n",
    "    \n",
    "    # Passando a seed\n",
    "    df_chaves_pix = _gerar_chaves_pix(df_contas_materializado, df_clientes_materializado, estado_ibge, municipio_ibge, seed=seed + 1)\n",
    "    salvar_dataframe_em_delta(df_chaves_pix, \"transacoes_db.copper.chaves_pix\", modo=\"append\")\n",
    "    \n",
    "    print(f\"INFO: População para o município {municipio_ibge} adicionada com sucesso.\")\n",
    "\n",
    "def limpar_tabelas_de_destino():\n",
    "    print(\"INFO: Apagando tabelas de destino para recriação...\")\n",
    "    for tabela in [\"clientes\", \"contas\", \"chaves_pix\", \"transacoes\"]:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS transacoes_db.copper.{tabela}\")\n",
    "    print(\"INFO: ✅ Limpeza concluída.\")\n",
    "\n",
    "def criar_tabelas_de_destino():\n",
    "    print(\"INFO: Criando tabelas de destino com os schemas corretos...\")\n",
    "    tabelas = {\n",
    "        \"transacoes_db.copper.clientes\": (SCHEMA_CLIENTES_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.contas\": (SCHEMA_CONTAS_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.chaves_pix\": (SCHEMA_CHAVES_PIX_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.transacoes\": (SCHEMA_TRANSACOES_FINAL, [\"estado_ibge\"])\n",
    "    }\n",
    "    for nome, (schema, part_cols) in tabelas.items():\n",
    "        spark.createDataFrame([], schema).write.format(\"delta\").partitionBy(part_cols).mode(\"overwrite\").saveAsTable(nome)\n",
    "        print(f\"INFO: Tabela '{nome}' criada com sucesso.\")\n",
    "\n",
    "print(f\"INFO: CÉLULA 2 (Refatorada V3) - Todas as funções foram definidas e corrigido o tipo da seed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4311c855-863d-4608-a3fe-c707e29a7e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA 3: ORQUESTRAÇÃO (REFATORADA E DETERMINÍSTICA)\n",
    "# =============================================================================\n",
    "try:\n",
    "    # 1. Preparação das Tabelas\n",
    "    limpar_tabelas_de_destino()\n",
    "    criar_tabelas_de_destino()\n",
    "    print(\"=============================================================================\")\n",
    "    print(f\"INFO: Iniciando processo de geração ANUAL (Ano: {ANO_ESTATISTICA})...\")\n",
    "    \n",
    "    # 2. Leitura e Seleção de Municípios\n",
    "    print(\"INFO: Lendo volumes anuais da tabela agregada...\")\n",
    "    df_volumes_anuais = spark.table(\"transacoes_db.pix_baseline_metricas.volumes_anuais_por_municipio\").filter(F.col(\"Ano\") == ANO_ESTATISTICA)\n",
    "    \n",
    "    print(\"INFO: Rankeando municípios para seleção...\")\n",
    "    # Rank é determinístico, não precisa de seed\n",
    "    df_ranks_anuais = df_volumes_anuais.withColumn(\"rank_pagador_anual\", F.rank().over(Window.orderBy(F.col(\"volume_pagador_anual\").desc())))\n",
    "    \n",
    "    municipios_a_processar_lista = df_ranks_anuais.orderBy(F.col(\"rank_pagador_anual\").asc()).limit(LIMITE_MUNICIPIOS_PROCESSADOS).collect()\n",
    "    total_municipios = len(municipios_a_processar_lista)\n",
    "    print(f\"INFO: {total_municipios} municípios selecionados para processar.\")\n",
    "    \n",
    "    id_municipios_selecionados = [row[\"cod_ibge_municipio\"] for row in municipios_a_processar_lista]\n",
    "\n",
    "    print(\"INFO: Coletando estatísticas mensais para os municípios selecionados...\")\n",
    "    # Coletar para o driver é uma boa otimização para < 100k linhas (Databricks Free)\n",
    "    stats_mensal_pd = (spark.table(\"transacoes_db.pix_baseline_metricas.relacao_pagadores_recebedores\")\n",
    "                          .filter((F.col(\"ano\") == ANO_ESTATISTICA) & (F.col(\"Municipio_Ibge\").isin(id_municipios_selecionados)))\n",
    "                          .select(\"Municipio_Ibge\", \"Mes\", \"total_tx_pf_pagador\", \"total_tx_pj_pagador\").toPandas())\n",
    "    stats_mensal_pd['total_tx_pagador'] = stats_mensal_pd['total_tx_pf_pagador'] + stats_mensal_pd['total_tx_pj_pagador']\n",
    "    print(\"INFO: Estatísticas mensais coletadas com sucesso.\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # PASSO 1: GERAR TODA A POPULAÇÃO PRIMEIRO\n",
    "    # =============================================================================\n",
    "    print(\"\\n--- PASSO 1: GERANDO POPULAÇÃO PARA TODOS OS MUNICÍPIOS ---\")\n",
    "    \n",
    "    municipios_ja_processados = [] \n",
    "\n",
    "    for i, municipio_row in enumerate(municipios_a_processar_lista):\n",
    "        codigo_municipio = municipio_row[\"cod_ibge_municipio\"]\n",
    "        codigo_estado = municipio_row[\"cod_ibge_estado\"]\n",
    "        nome_municipio = municipio_row[\"municipio_nome\"]\n",
    "        \n",
    "        print(f\"\\nGerando População {i+1}/{total_municipios}: {nome_municipio} ({codigo_municipio})\")\n",
    "        \n",
    "        fator_escala_final = FATOR_ESCALA_VOLUME\n",
    "        volume_pf_anual = int(municipio_row[\"total_pf_anual\"] * fator_escala_final)\n",
    "        volume_pj_anual = int(municipio_row[\"total_pj_anual\"] * fator_escala_final)\n",
    "        num_pf = max(1, int(volume_pf_anual / (TX_POR_CLIENTE_ESPERADO * 12)))\n",
    "        num_pj = max(1, int(volume_pj_anual / (TX_POR_CLIENTE_ESPERADO * 12)))\n",
    "\n",
    "        # ! REATORAÇÃO: Passando seed única por município\n",
    "        gerar_e_salvar_populacao(\n",
    "            num_pf=num_pf, num_pj=num_pj, \n",
    "            estado_ibge=codigo_estado, municipio_ibge=codigo_municipio,\n",
    "            seed=GLOBAL_SEED + i # Seed única\n",
    "        )\n",
    "        municipios_ja_processados.append(codigo_municipio)\n",
    "\n",
    "    print(\"\\n--- PASSO 1 CONCLUÍDO: Todas as populações foram salvas. ---\")\n",
    "  \n",
    "    print(\"\\n--- PASSO 2 (REMOVIDO): Pool de contas será gerado dentro da UDF. ---\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # PASSO 3: GERAR TRANSAÇÕES (SEM CACHE - Otimizado)\n",
    "    # =============================================================================\n",
    "    print(\"\\n--- PASSO 3: GERANDO TRANSAÇÕES MÊS A MÊS ---\")\n",
    "\n",
    "    for i, municipio_row in enumerate(municipios_a_processar_lista):\n",
    "        codigo_municipio = municipio_row[\"cod_ibge_municipio\"]\n",
    "        codigo_estado = municipio_row[\"cod_ibge_estado\"]\n",
    "        nome_municipio = municipio_row[\"municipio_nome\"]\n",
    "        \n",
    "        print(f\"\\n================== Iniciando Transações {i+1}/{total_municipios}: {nome_municipio} ({codigo_municipio}) ==================\")\n",
    "        \n",
    "        fator_escala_final = FATOR_ESCALA_VOLUME\n",
    "\n",
    "        # Esta é a leitura otimizada (fora do loop mensal)\n",
    "        print(\"INFO: Lendo dados do município para o loop mensal (sem cache)...\")\n",
    "        df_contas_do_municipio = spark.table(\"transacoes_db.copper.contas\") \\\n",
    "                                      .filter(F.col(\"municipio_ibge\") == codigo_municipio)\n",
    "        \n",
    "        df_clientes_do_municipio = spark.table(\"transacoes_db.copper.clientes\") \\\n",
    "                                        .filter(F.col(\"municipio_ibge\") == codigo_municipio)\n",
    "        \n",
    "        num_contas_do_municipio = df_contas_do_municipio.count()\n",
    "        if num_contas_do_municipio == 0:\n",
    "            print(f\"AVISO: Nenhuma conta encontrada para {nome_municipio}. Pulando...\")\n",
    "            continue\n",
    "        \n",
    "        df_chaves_do_municipio = spark.table(\"transacoes_db.copper.chaves_pix\") \\\n",
    "                                       .filter(F.col(\"municipio_ibge\") == codigo_municipio)\n",
    "        \n",
    "        window_chaves = Window.partitionBy(\"id_conta\").orderBy(F.col(\"cadastrada_em\").desc())\n",
    "        \n",
    "        df_chaves_recentes_do_municipio = (df_chaves_do_municipio\n",
    "                                            .withColumn(\"rank\", F.rank().over(window_chaves))\n",
    "                                            .filter(F.col(\"rank\") == 1)\n",
    "                                            .select(\"id_conta\", F.col(\"cadastrada_em\").alias(\"chave_destino_cadastrada_em\")))\n",
    "\n",
    "        for mes in range(1, 13):\n",
    "            print(f\"\\n--- Processando Mês {mes}/{ANO_ESTATISTICA} para {nome_municipio} ---\")\n",
    "            \n",
    "            stats_mensal = stats_mensal_pd[(stats_mensal_pd['Municipio_Ibge'] == codigo_municipio) & (stats_mensal_pd['Mes'] == mes)]\n",
    "            if stats_mensal.empty: \n",
    "                print(f\"AVISO: Sem estatísticas para {mes}/{ANO_ESTATISTICA}. Pulando.\"); continue\n",
    "            \n",
    "            volume_total_original = stats_mensal[\"total_tx_pagador\"].iloc[0]\n",
    "            volume_total = int(volume_total_original * fator_escala_final)\n",
    "            if volume_total <= 0: \n",
    "                print(f\"AVISO: Volume de transações base para {mes}/{ANO_ESTATISTICA} é 0. Pulando.\"); continue\n",
    "                \n",
    "            print(f\"       Volume Original: {volume_total_original} | Volume BASE Alvo: {volume_total}\")\n",
    "            \n",
    "            print(f\"        INFO: Gerando perfil de sazonalidade para o Mês {mes}...\")\n",
    "            perfil_sazonal_mes = _obter_perfil_sazonal_mes(ANO_ESTATISTICA, mes)\n",
    "            \n",
    "            # ! REATORAÇÃO: Passando seed única (município + mês)\n",
    "            df_transacoes = gerar_transacoes(\n",
    "                df_contas_local=df_contas_do_municipio,\n",
    "                df_clientes_local=df_clientes_do_municipio,\n",
    "                df_chaves_recentes_local=df_chaves_recentes_do_municipio,\n",
    "                num_contas_local=num_contas_do_municipio,\n",
    "                volume_total=volume_total, \n",
    "                estado_ibge=codigo_estado, \n",
    "                municipio_ibge=codigo_municipio,\n",
    "                ano=ANO_ESTATISTICA, \n",
    "                mes=mes, \n",
    "                municipios_processados=municipios_ja_processados,\n",
    "                perfil_sazonal=perfil_sazonal_mes,\n",
    "                seed=GLOBAL_SEED + i + mes # Seed única por task\n",
    "            )\n",
    "            salvar_dataframe_em_delta(df_transacoes, \"transacoes_db.copper.transacoes\", modo=\"append\")\n",
    "        \n",
    "        print(f\"INFO: Processamento do município {nome_municipio} concluído.\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nINFO: O script chegou ao fim.\")\n",
    "\n",
    "print(\"\\n=============================================================================\")\n",
    "print(\"INFO: Processo de geração de dados sintéticos (determinístico) concluído.\")\n",
    "print(\"=============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5646dac-50ab-4629-a057-f987bbd69392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA: DIAGNÓSTICO PRÉ-BALANCEAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"INFO: Analisando a distribuição da tabela ORIGINAL (copper.transacoes)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Carrega a tabela original\n",
    "    df_original = spark.table(\"transacoes_db.copper.transacoes\")\n",
    "    \n",
    "    # 2. Calcula o total de registros\n",
    "    total_count = df_original.count()\n",
    "    \n",
    "    if total_count == 0:\n",
    "        print(\"AVISO: A tabela original está vazia. Nenhuma análise para mostrar.\")\n",
    "    else:\n",
    "        print(f\"Total de Registros (Original): {total_count}\\n\")\n",
    "        \n",
    "        # 3. Calcula a distribuição por 'is_fraud'\n",
    "        print(\"Distribuição por Classe (Original):\")\n",
    "        df_original.groupBy(\"is_fraud\") \\\n",
    "            .count() \\\n",
    "            .withColumn(\"percentual\", (F.col(\"count\") / total_count) * 100) \\\n",
    "            .orderBy(F.col(\"count\").desc()) \\\n",
    "            .show()\n",
    "\n",
    "        # 4. Calcula a distribuição detalhada por 'fraud_type'\n",
    "        print(\"Distribuição por Tipo de Fraude (Original):\")\n",
    "        df_original.withColumn(\n",
    "                \"tipo_fraude_detalhado\", \n",
    "                F.when(F.col(\"is_fraud\") == 0, \"Legitima\").otherwise(F.col(\"fraud_type\"))\n",
    "            ) \\\n",
    "            .groupBy(\"tipo_fraude_detalhado\") \\\n",
    "            .count() \\\n",
    "            .withColumn(\"percentual\", (F.col(\"count\") / total_count) * 100) \\\n",
    "            .orderBy(F.col(\"count\").desc()) \\\n",
    "            .show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível ler 'transacoes_db.copper.transacoes'. Verifique se a Célula 3 foi executada.\")\n",
    "    print(f\"Detalhe: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21005f4-56aa-4fb4-9bb5-f56997d28860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA 5: BALANCEAMENTO (VERSÃO V11: CORREÇÃO DE ERRO DE FRAÇÃO + SEM CACHE)\n",
    "# (Corrige o IllegalArgumentException ao balancear corretamente a classe majoritária)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"INFO: Iniciando processo de BALANCEAMENTO V11 (Robusto)...\")\n",
    "print(\"INFO: Classe 1 (Fraude) incluirá a Raiz E os Sintomas (filhos da cadeia).\")\n",
    "print(\"INFO: Classe 0 (Legítimo) incluirá APENAS transações legítimas.\")\n",
    "print(\"AVISO: Executando em modo 'SEM CACHE' por instrução. A execução pode ser lenta.\")\n",
    "\n",
    "# Usar a mesma seed global para reprodutibilidade\n",
    "SEED_BALANCEAMENTO = 42\n",
    "\n",
    "try:\n",
    "    df_transacoes = spark.table(\"transacoes_db.copper.transacoes\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # PASSO 1: Isolar as \"Fraudes\" (Classe 1 - Causa + Sintomas)\n",
    "    # =========================================================================\n",
    "    df_fraudes_all = df_transacoes.filter(\n",
    "        F.col(\"is_fraud\") == 1\n",
    "    ) # SEM .cache()\n",
    "    \n",
    "    count_fraudes_all = df_fraudes_all.count()\n",
    "\n",
    "    if count_fraudes_all == 0:\n",
    "        raise ValueError(\"ERRO CRÍTICO: Nenhuma transação com 'is_fraud = 1' foi encontrada.\")\n",
    "    \n",
    "    print(f\"INFO: Total de 'Fraudes' (Classe 1 - Original): {count_fraudes_all} linhas.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # PASSO 2: Isolar as \"Legítimas\" (Classe 0)\n",
    "    # =========================================================================\n",
    "    \n",
    "    df_legitimas_all = df_transacoes.filter(\n",
    "        F.col(\"is_fraud\") == 0\n",
    "    ) # SEM .cache()\n",
    "        \n",
    "    count_legitimas_all = df_legitimas_all.count()\n",
    "    \n",
    "    if count_legitimas_all == 0:\n",
    "        raise ValueError(\"ERRO CRÍTICO: Nenhuma transação com 'is_fraud = 0' foi encontrada.\")\n",
    "        \n",
    "    print(f\"INFO: Total de 'Legítimas' (Classe 0): {count_legitimas_all} linhas.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # PASSO 3: APLICAR OVERSAMPLING (Sobreamostragem) no bloco de 'Fraudes'\n",
    "    # =========================================================================\n",
    "    print(\"INFO: Iniciando Etapa 1: Oversampling Multiclasse (Balanceando tipos de fraude)...\")\n",
    "\n",
    "    # 3a. Encontrar a contagem da classe de fraude majoritária\n",
    "    df_counts = df_fraudes_all.groupBy(\"fraud_type\").count()\n",
    "    \n",
    "    # Coletar a contagem máxima para o driver\n",
    "    target_count_multiclass = df_counts.select(F.max(\"count\")).first()[0]\n",
    "    \n",
    "    print(f\"INFO: Classe de fraude majoritária tem {target_count_multiclass} amostras. Este é o alvo.\")\n",
    "    print(\"Distribuição de Fraudes (Antes do Oversampling):\")\n",
    "    df_counts.show()\n",
    "\n",
    "    # 3b. Calcular o fator de repetição para cada classe\n",
    "    df_fraudes_com_fator = df_fraudes_all.join(\n",
    "        df_counts.withColumn(\"target_count\", F.lit(target_count_multiclass)),\n",
    "        \"fraud_type\"\n",
    "    ).withColumn(\n",
    "        \"repeat_n\", \n",
    "        F.ceil(F.col(\"target_count\") / F.col(\"count\")).cast(\"int\") # Fator de repetição\n",
    "    )\n",
    "\n",
    "    # 3c. Explodir para duplicar as linhas das classes minoritárias\n",
    "    df_oversampled_base = df_fraudes_com_fator.withColumn(\n",
    "        \"copy_index\", \n",
    "        F.explode(F.sequence(F.lit(1), F.col(\"repeat_n\")))\n",
    "    )\n",
    "\n",
    "    # 3d. RE-GERAR IDs para evitar chaves duplicadas (APENAS PARA CÓPIAS)\n",
    "    df_fraudes_oversampled_com_novos_ids = df_oversampled_base.withColumn(\n",
    "        \"id_original\", F.col(\"id\") # Salva o ID original para referência\n",
    "    ).withColumn(\n",
    "        \"id\",\n",
    "        F.when(\n",
    "            F.col(\"copy_index\") > 1, F.expr(\"uuid()\") # Gera novo ID para cópias\n",
    "        ).otherwise(F.col(\"id\")) # Mantém o ID original\n",
    "    ).withColumn(\n",
    "        \"id_transacao_cadeia_pai\",\n",
    "        F.when(\n",
    "            F.col(\"copy_index\") > 1, F.lit(None) # Cópias não são \"filhas\"\n",
    "        ).otherwise(F.col(\"id_transacao_cadeia_pai\")) # Mantém o pai original\n",
    "    ).drop(\"count\", \"target_count\", \"repeat_n\", \"copy_index\", \"id_original\")\n",
    "    \n",
    "    \n",
    "    # 3e. Truncar (sample) de volta ao 'target_count'\n",
    "    window_spec = Window.partitionBy(\"fraud_type\").orderBy(F.rand(seed=SEED_BALANCEAMENTO))\n",
    "    \n",
    "    df_fraudes_balanceadas_multiclass = df_fraudes_oversampled_com_novos_ids \\\n",
    "        .withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"rank\") <= target_count_multiclass) \\\n",
    "        .drop(\"rank\") # SEM .cache()\n",
    "\n",
    "    count_fraudes_balanceadas = df_fraudes_balanceadas_multiclass.count()\n",
    "    print(f\"INFO: Fraudes após oversampling (Etapa 1): {count_fraudes_balanceadas} linhas.\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # --- INÍCIO DA CORREÇÃO (PASSO 4 V11) ---\n",
    "    # PASSO 4: BALANCEAMENTO BINÁRIO ROBUSTO (Undersampling da Classe Majoritária)\n",
    "    # =========================================================================\n",
    "    print(\"INFO: Iniciando Etapa 2: Balanceamento Binário (Identificando a classe majoritária)...\")\n",
    "    \n",
    "    # count_fraudes_balanceadas (do PASSO 3)\n",
    "    # count_legitimas_all (do PASSO 2)\n",
    "\n",
    "    if count_fraudes_balanceadas == count_legitimas_all:\n",
    "        print(f\"INFO: Classes já estão balanceadas (1:1) com {count_fraudes_balanceadas} amostras cada. Nenhuma amostragem binária necessária.\")\n",
    "        df_fraudes_amostradas = df_fraudes_balanceadas_multiclass\n",
    "        df_legitimas_amostradas = df_legitimas_all\n",
    "    \n",
    "    elif count_fraudes_balanceadas > count_legitimas_all:\n",
    "        # CASO 1: Fraude é a MAIORIA (Aplicar Undersampling nas Fraudes)\n",
    "        # Isso acontece em datasets de baixa escala onde o oversampling de fraudes supera as legítimas.\n",
    "        print(f\"INFO: Classe 'Fraude' é a majoritária ({count_fraudes_balanceadas} vs {count_legitimas_all}).\")\n",
    "        print(\"INFO: Aplicando UNDERSAMPLING na classe 'Fraude'...\")\n",
    "        \n",
    "        fraction = count_legitimas_all / count_fraudes_balanceadas # (Será < 1.0)\n",
    "        print(f\"INFO: Fração de amostragem (Fraude): {fraction:.4f}\")\n",
    "        \n",
    "        df_fraudes_amostradas = df_fraudes_balanceadas_multiclass.sample(\n",
    "            withReplacement=False, \n",
    "            fraction=fraction, \n",
    "            seed=SEED_BALANCEAMENTO\n",
    "        )\n",
    "        df_legitimas_amostradas = df_legitimas_all # Manter todas as legítimas\n",
    "\n",
    "    else:\n",
    "        # CASO 2: Legítima é a MAIORIA (Aplicar Undersampling nas Legítimas)\n",
    "        # Este é o cenário esperado em produção.\n",
    "        print(f\"INFO: Classe 'Legítima' é a majoritária ({count_legitimas_all} vs {count_fraudes_balanceadas}).\")\n",
    "        print(\"INFO: Aplicando UNDERSAMPLING na classe 'Legítima'...\")\n",
    "        \n",
    "        fraction = count_fraudes_balanceadas / count_legitimas_all # (Será < 1.0)\n",
    "        print(f\"INFO: Fração de amostragem (Legítima): {fraction:.4f}\")\n",
    "        \n",
    "        df_legitimas_amostradas = df_legitimas_all.sample(\n",
    "            withReplacement=False, \n",
    "            fraction=fraction, \n",
    "            seed=SEED_BALANCEAMENTO\n",
    "        )\n",
    "        df_fraudes_amostradas = df_fraudes_balanceadas_multiclass # Manter todas as fraudes\n",
    "    \n",
    "    # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "\n",
    "    # =========================================================================\n",
    "    # PASSO 5: Unir e Salvar o Dataset Final\n",
    "    # =========================================================================\n",
    "    df_transacoes_balanced = df_fraudes_amostradas.unionByName(df_legitimas_amostradas)\n",
    "\n",
    "    print(\"INFO: Materializando dataset balanceado (V11-Robusto) como 'transacoes_db.gold.transacoes_balanced_model'...\")\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS transacoes_db.gold\")\n",
    "    \n",
    "    df_transacoes_balanced.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(\"transacoes_db.gold.transacoes_balanced_model\")\n",
    "    \n",
    "    print(\"✅ SUCESSO: Dataset balanceado (V11-Robusto) salvo.\")\n",
    "\n",
    "finally:\n",
    "    # Bloco 'finally' agora está seguro e não tenta limpar caches\n",
    "    print(\"INFO: Processo finalizado (sem caches para limpar).\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CÉLULA 6 (Verificação) - Execute esta após a Célula 5\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n--- VERIFICAÇÃO PÓS-BALANCEAMENTO (V11-ROBUSTO) ---\")\n",
    "df_check = spark.table(\"transacoes_db.gold.transacoes_balanced_model\")\n",
    "\n",
    "\n",
    "total_count_balanced = df_check.count()\n",
    "print(f\"Total de Registros (Balanceado): {total_count_balanced}\")\n",
    "\n",
    "print(\"\\nDistribuição por Classe (Balanceado):\")\n",
    "df_check.groupBy(\"is_fraud\").count().show()\n",
    "# Esperado: 50/50 (ou muito próximo)\n",
    "\n",
    "print(\"\\nDistribuição por Tipo de Fraude (Balanceado):\")\n",
    "df_check.withColumn(\n",
    "        \"tipo_fraude_detalhado\", \n",
    "        F.when(F.col(\"is_fraud\") == 0, \"Legitima\").otherwise(F.col(\"fraud_type\"))\n",
    "    ) \\\n",
    "    .groupBy(\"tipo_fraude_detalhado\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.col(\"count\").desc()) \\\n",
    "    .show()\n",
    "# Esperado: 'Legitima' (50%) e todos os tipos de fraude\n",
    "# (ex: 'valor_atipico', 'eng_social', etc.) com contagens IDÊNTICAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ad1024-e1be-48fa-85ff-161d72dc3511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA: DIAGNÓSTICO PÓS-BALANCEAMENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"INFO: Analisando a distribuição da tabela BALANCEADA (gold.transacoes_balanced_model)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Carrega a tabela balanceada\n",
    "    df_balanced = spark.table(\"transacoes_db.gold.transacoes_balanced_model\")\n",
    "    \n",
    "    # 2. Calcula o total de registros\n",
    "    total_count_balanced = df_balanced.count()\n",
    "    \n",
    "    if total_count_balanced == 0:\n",
    "        print(\"AVISO: A tabela balanceada está vazia. Verifique se a célula de undersampling foi executada.\")\n",
    "    else:\n",
    "        print(f\"Total de Registros (Balanceado): {total_count_balanced}\\n\")\n",
    "        \n",
    "        # 3. Calcula a distribuição por 'is_fraud'\n",
    "        print(\"Distribuição por Classe (Balanceado):\")\n",
    "        df_balanced.groupBy(\"is_fraud\") \\\n",
    "            .count() \\\n",
    "            .withColumn(\"percentual\", (F.col(\"count\") / total_count_balanced) * 100) \\\n",
    "            .orderBy(F.col(\"count\").desc()) \\\n",
    "            .show()\n",
    "\n",
    "        # 4. Calcula a distribuição detalhada por 'fraud_type'\n",
    "        print(\"Distribuição por Tipo de Fraude (Balanceado):\")\n",
    "        # Note que os tipos de fraude (classe 1) terão suas contagens reduzidas\n",
    "        df_balanced.withColumn(\n",
    "                \"tipo_fraude_detalhado\", \n",
    "                F.when(F.col(\"is_fraud\") == 0, \"Legitima\").otherwise(F.col(\"fraud_type\"))\n",
    "            ) \\\n",
    "            .groupBy(\"tipo_fraude_detalhado\") \\\n",
    "            .count() \\\n",
    "            .withColumn(\"percentual\", (F.col(\"count\") / total_count_balanced) * 100) \\\n",
    "            .orderBy(F.col(\"count\").desc()) \\\n",
    "            .show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível ler 'transacoes_db.gold.transacoes_balanced_model'. Verifique se a célula de undersampling foi executada com sucesso.\")\n",
    "    print(f\"Detalhe: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f2d1de55-4f3c-44ad-a1b1-1d3fc425bf65",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761099536969}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE transacoes_db.gold.transacoes_dataset\n",
    "SELECT\n",
    "  -- Colunas da transação\n",
    "  tx.id AS transacao_id,\n",
    "  tx.valor AS valor_transacao,\n",
    "  tx.data AS data_transacao,\n",
    "  tx.mensagem AS mensagem_pix,\n",
    "  tx.id_conta_origem AS id_conta_pagador,\n",
    "  tx.id_conta_destino AS id_conta_recebedor,\n",
    "  tx.id_tipo_iniciacao_pix AS tipo_iniciacao_pix_id,\n",
    "  tx.id_finalidade_pix AS finalidade_pix_id,\n",
    "  tx.is_fraud AS transacao_fraudulenta,\n",
    "  tx.fraud_type AS tipo_fraude,\n",
    "  tx.id_transacao_cadeia_pai AS id_transacao_cadeia_pai,\n",
    "  tx.estado_ibge AS estado_ibge_transacao,\n",
    "\n",
    "  -- Pagador: Conta\n",
    "  conta_orig.id AS pagador_conta_id,\n",
    "  conta_orig.saldo AS pagador_saldo,\n",
    "  conta_orig.aberta_em AS pagador_conta_aberta_em,\n",
    "  conta_orig.agencia AS pagador_agencia,\n",
    "  conta_orig.numero AS pagador_numero_conta,\n",
    "  conta_orig.id_tipo_conta AS pagador_tipo_conta_id,\n",
    "  conta_orig.ispb_instituicao AS pagador_ispb_instituicao,\n",
    "  conta_orig.id_cliente AS pagador_cliente_id_conta,\n",
    "  conta_orig.is_high_risk AS pagador_conta_alto_risco,\n",
    "  conta_orig.estado_ibge AS pagador_estado_ibge,\n",
    "  conta_orig.municipio_ibge AS pagador_municipio_ibge,\n",
    "\n",
    "  -- Pagador: Cliente\n",
    "  cliente_orig.id AS pagador_cliente_id,\n",
    "  cliente_orig.nome AS pagador_nome,\n",
    "  cliente_orig.id_natureza AS pagador_natureza_id,\n",
    "  cliente_orig.registro_nacional AS pagador_registro_nacional,\n",
    "  cliente_orig.nascido_em AS pagador_data_nascimento,\n",
    "  cliente_orig.estado_ibge AS pagador_estado_ibge_cliente,\n",
    "  cliente_orig.municipio_ibge AS pagador_municipio_ibge_cliente,\n",
    "\n",
    "  -- Pagador: Instituição\n",
    "  inst_orig.ispb AS pagador_instituicao_ispb,\n",
    "  inst_orig.nome AS pagador_instituicao,\n",
    "\n",
    "  -- Pagador: Tipo de Conta\n",
    "  tipo_conta_orig.id AS pagador_tipo_conta_id_ref,\n",
    "  tipo_conta_orig.nome AS pagador_tipo_conta_descricao,\n",
    "\n",
    "  -- Pagador: Município\n",
    "  mun_orig.codigo_ibge AS pagador_municipio_ibge_ref,\n",
    "  mun_orig.nome AS pagador_municipio,\n",
    "\n",
    "  -- Pagador: Natureza\n",
    "  natureza_orig.id AS pagador_natureza_id_ref,\n",
    "  natureza_orig.nome AS pagador_natureza,\n",
    "\n",
    "  -- Recebedor: Conta\n",
    "  conta_dest.id AS recebedor_conta_id,\n",
    "  conta_dest.saldo AS recebedor_saldo,\n",
    "  conta_dest.aberta_em AS recebedor_conta_aberta_em,\n",
    "  conta_dest.agencia AS recebedor_agencia,\n",
    "  conta_dest.numero AS recebedor_numero_conta,\n",
    "  conta_dest.id_tipo_conta AS recebedor_tipo_conta_id,\n",
    "  conta_dest.ispb_instituicao AS recebedor_ispb_instituicao,\n",
    "  conta_dest.id_cliente AS recebedor_cliente_id_conta,\n",
    "  conta_dest.is_high_risk AS recebedor_conta_alto_risco,\n",
    "  conta_dest.estado_ibge AS recebedor_estado_ibge,\n",
    "  conta_dest.municipio_ibge AS recebedor_municipio_ibge,\n",
    "\n",
    "  -- Recebedor: Cliente\n",
    "  cliente_dest.id AS recebedor_cliente_id,\n",
    "  cliente_dest.nome AS recebedor_nome,\n",
    "  cliente_dest.id_natureza AS recebedor_natureza_id,\n",
    "  cliente_dest.registro_nacional AS recebedor_registro_nacional,\n",
    "  cliente_dest.nascido_em AS recebedor_data_nascimento,\n",
    "  cliente_dest.estado_ibge AS recebedor_estado_ibge_cliente,\n",
    "  cliente_dest.municipio_ibge AS recebedor_municipio_ibge_cliente,\n",
    "\n",
    "  -- Recebedor: Instituição\n",
    "  inst_dest.ispb AS recebedor_instituicao_ispb,\n",
    "  inst_dest.nome AS recebedor_instituicao,\n",
    "\n",
    "\n",
    "  -- Recebedor: Tipo de Conta\n",
    "  tipo_conta_dest.id AS recebedor_tipo_conta_id_ref,\n",
    "  tipo_conta_dest.nome AS recebedor_tipo_conta_descricao,\n",
    "\n",
    "  -- Recebedor: Município\n",
    "  mun_dest.codigo_ibge AS recebedor_municipio_ibge_ref,\n",
    "  mun_dest.nome AS recebedor_municipio,\n",
    "\n",
    "  -- Recebedor: Natureza\n",
    "  natureza_dest.id AS recebedor_natureza_id_ref,\n",
    "  natureza_dest.nome AS recebedor_natureza\n",
    "\n",
    "\n",
    "FROM\n",
    "  transacoes_db.copper.transacoes AS tx\n",
    "\n",
    "LEFT JOIN transacoes_db.copper.contas AS conta_orig\n",
    "  ON tx.id_conta_origem = conta_orig.id\n",
    "LEFT JOIN transacoes_db.copper.clientes AS cliente_orig\n",
    "  ON conta_orig.id_cliente = cliente_orig.id\n",
    "LEFT JOIN transacoes_db.copper.instituicoes AS inst_orig\n",
    "  ON conta_orig.ispb_instituicao = inst_orig.ispb\n",
    "LEFT JOIN transacoes_db.copper.tipos_conta AS tipo_conta_orig\n",
    "  ON conta_orig.id_tipo_conta = tipo_conta_orig.id\n",
    "LEFT JOIN transacoes_db.copper.municipios AS mun_orig\n",
    "  ON cliente_orig.municipio_ibge = mun_orig.codigo_ibge\n",
    "LEFT JOIN transacoes_db.copper.naturezas AS natureza_orig\n",
    "  ON cliente_orig.id_natureza = natureza_orig.id\n",
    "\n",
    "LEFT JOIN transacoes_db.copper.contas AS conta_dest\n",
    "  ON tx.id_conta_destino = conta_dest.id\n",
    "LEFT JOIN transacoes_db.copper.clientes AS cliente_dest\n",
    "  ON conta_dest.id_cliente = cliente_dest.id\n",
    "LEFT JOIN transacoes_db.copper.instituicoes AS inst_dest\n",
    "  ON conta_dest.ispb_instituicao = inst_dest.ispb\n",
    "LEFT JOIN transacoes_db.copper.tipos_conta AS tipo_conta_dest\n",
    "  ON conta_dest.id_tipo_conta = tipo_conta_dest.id\n",
    "LEFT JOIN transacoes_db.copper.municipios AS mun_dest\n",
    "  ON cliente_dest.municipio_ibge = mun_dest.codigo_ibge\n",
    "LEFT JOIN transacoes_db.copper.naturezas AS natureza_dest\n",
    "  ON cliente_dest.id_natureza = natureza_dest.id\n",
    "\n",
    "LEFT JOIN transacoes_db.copper.finalidade_pix AS finalidade_pix\n",
    "  ON tx.id_finalidade_pix = finalidade_pix.id\n",
    "\n",
    "ORDER BY\n",
    "  tx.data DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ffc717b-af19-435f-b6c4-d6e1e5727bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW transacoes_db.feature_store.in_live_features AS\n",
    "WITH tx_com_long AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    CAST(data AS LONG) AS data_em_segundos\n",
    "  FROM\n",
    "    transacoes_db.gold.transacoes_balanced_model\n",
    "),\n",
    "tx_features_realtime AS (\n",
    "  SELECT\n",
    "    tx.*,\n",
    "    -- Pagador: últimas 24h\n",
    "    COUNT(1) OVER (\n",
    "      PARTITION BY id_conta_origem\n",
    "      ORDER BY data_em_segundos\n",
    "      RANGE BETWEEN (24 * 3600) PRECEDING AND 1 PRECEDING\n",
    "    ) AS pagador_txs_ultimas_24h,\n",
    "    COALESCE(\n",
    "      SUM(valor) OVER (\n",
    "        PARTITION BY id_conta_origem\n",
    "        ORDER BY data_em_segundos\n",
    "        RANGE BETWEEN (24 * 3600) PRECEDING AND 1 PRECEDING\n",
    "      ), 0\n",
    "    ) AS pagador_valor_ultimas_24h,\n",
    "    -- Recebedor: última 1h\n",
    "    COUNT(1) OVER (\n",
    "      PARTITION BY id_conta_destino\n",
    "      ORDER BY data_em_segundos\n",
    "      RANGE BETWEEN 3600 PRECEDING AND 1 PRECEDING\n",
    "    ) AS recebedor_txs_ultima_1h,\n",
    "    COALESCE(\n",
    "      SUM(valor) OVER (\n",
    "        PARTITION BY id_conta_destino\n",
    "        ORDER BY data_em_segundos\n",
    "        RANGE BETWEEN 3600 PRECEDING AND 1 PRECEDING\n",
    "      ), 0\n",
    "    ) AS recebedor_valor_ultima_1h,\n",
    "    -- Lag do pagador\n",
    "    data_em_segundos - LAG(data_em_segundos, 1, 0) OVER (\n",
    "      PARTITION BY id_conta_origem\n",
    "      ORDER BY data_em_segundos\n",
    "    ) AS pagador_segundos_desde_ultima_tx\n",
    "  FROM\n",
    "    tx_com_long tx\n",
    ")\n",
    "SELECT\n",
    "  -- Colunas da transação\n",
    "  ft.valor AS valor_transacao,\n",
    "  ft.data AS data_transacao,\n",
    "  ft.id_conta_origem AS id_conta_pagador,\n",
    "  ft.id_conta_destino AS id_conta_recebedor,\n",
    "  ft.id_tipo_iniciacao_pix AS tipo_iniciacao_pix_id,\n",
    "  ft.id_finalidade_pix AS finalidade_pix_id,\n",
    "  ft.is_fraud AS transacao_fraudulenta,\n",
    "  ft.fraud_type AS tipo_fraude,\n",
    "\n",
    "  -- Pagador: Perfil\n",
    "  conta_orig.saldo AS pagador_saldo,\n",
    "  conta_orig.aberta_em AS pagador_conta_aberta_em,\n",
    "  conta_orig.id_tipo_conta AS pagador_tipo_conta_id,\n",
    "  cliente_orig.id_natureza AS pagador_natureza_id,\n",
    "  cliente_orig.nascido_em AS pagador_data_nascimento,\n",
    "\n",
    "  -- Recebedor: Perfil\n",
    "  conta_dest.saldo AS recebedor_saldo,\n",
    "  conta_dest.aberta_em AS recebedor_conta_aberta_em,\n",
    "  conta_dest.id_tipo_conta AS recebedor_tipo_conta_id,\n",
    "  cliente_dest.id_natureza AS recebedor_natureza_id,\n",
    "  cliente_dest.nascido_em AS recebedor_data_nascimento,\n",
    "\n",
    "  -- Features de tempo real\n",
    "  ft.pagador_txs_ultimas_24h,\n",
    "  ft.pagador_valor_ultimas_24h,\n",
    "  ft.recebedor_txs_ultima_1h,\n",
    "  ft.recebedor_valor_ultima_1h,\n",
    "  ft.pagador_segundos_desde_ultima_tx\n",
    "FROM\n",
    "  tx_features_realtime AS ft\n",
    "  LEFT JOIN transacoes_db.copper.contas AS conta_orig\n",
    "    ON ft.id_conta_origem = conta_orig.id\n",
    "  LEFT JOIN transacoes_db.copper.clientes AS cliente_orig\n",
    "    ON conta_orig.id_cliente = cliente_orig.id\n",
    "  LEFT JOIN transacoes_db.copper.contas AS conta_dest\n",
    "    ON ft.id_conta_destino = conta_dest.id\n",
    "  LEFT JOIN transacoes_db.copper.clientes AS cliente_dest\n",
    "    ON conta_dest.id_cliente = cliente_dest.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "136ebbeb-bbd0-49e9-90e5-41e4de2ea502",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"data_transacao\":317},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762137157921}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW transacoes_db.feature_store.historical_data_features AS \n",
    "-- Bloco 1: CTE para pré-agregar as chaves PIX\n",
    "-- Isso previne a duplicação de linhas na junção principal e melhora a performance.\n",
    "WITH contas_enriquecidas AS (\n",
    "  SELECT\n",
    "    c.id,\n",
    "    c.id_cliente,\n",
    "    c.saldo,\n",
    "    c.aberta_em,\n",
    "    c.id_tipo_conta,\n",
    "    c.ispb_instituicao,\n",
    "    c.estado_ibge,\n",
    "    c.municipio_ibge,\n",
    "    -- Nós NÃO usamos 'is_high_risk' para evitar vazamento de dados\n",
    "    \n",
    "    -- Agregações da tabela de chaves\n",
    "    coalesce(k.qtd_chaves, 0) AS qtd_chaves,\n",
    "    k.primeira_chave_em,\n",
    "    k.ultima_chave_em\n",
    "  FROM\n",
    "    transacoes_db.copper.contas c\n",
    "  LEFT JOIN (\n",
    "    -- Subquery que calcula as métricas por conta\n",
    "    SELECT\n",
    "      id_conta,\n",
    "      COUNT(id) AS qtd_chaves,\n",
    "      MIN(cadastrada_em) AS primeira_chave_em,\n",
    "      MAX(cadastrada_em) AS ultima_chave_em\n",
    "    FROM\n",
    "      transacoes_db.copper.chaves_pix\n",
    "    GROUP BY\n",
    "      id_conta\n",
    "  ) k ON c.id = k.id_conta\n",
    ")\n",
    "\n",
    "-- Bloco 2: Query Principal (Engenharia de Features)\n",
    "SELECT\n",
    "  -- ===================================================================\n",
    "  -- 1. TARGETS (As respostas que a IA deve aprender)\n",
    "  -- ===================================================================\n",
    "  tx.is_fraud AS transacao_fraudulenta,\n",
    "  coalesce(tx.fraud_type, 'Legitima') AS tipo_fraude,\n",
    "  \n",
    "  -- ===================================================================\n",
    "  -- 2. FEATURES BASE (Dados brutos da transação)\n",
    "  -- ===================================================================\n",
    "  tx.valor AS valor_transacao,\n",
    "  tx.data AS data_transacao,\n",
    "  coalesce(tx.id_tipo_iniciacao_pix, -1) AS tipo_iniciacao_pix_id,\n",
    "  coalesce(tx.id_finalidade_pix, -1) AS finalidade_pix_id,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 3. FEATURES DE VELOCIDADE (Quão rápido as coisas acontecem?)\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: Nenhuma necessária. Esta lógica já retorna 0 em vez de NULL.\n",
    "  hour(tx.data) AS hora_do_dia,\n",
    "  \n",
    "  (unix_timestamp(tx.data) - unix_timestamp(\n",
    "      LAG(tx.data, 1, tx.data) OVER (\n",
    "        PARTITION BY tx.id_conta_origem \n",
    "        ORDER BY tx.data ASC\n",
    "      )\n",
    "  )) AS pagador_segundos_desde_ult_tx,\n",
    "  \n",
    "  (unix_timestamp(tx.data) - unix_timestamp(\n",
    "      LAG(tx.data, 1, tx.data) OVER (\n",
    "        PARTITION BY tx.id_conta_destino \n",
    "        ORDER BY tx.data ASC\n",
    "      )\n",
    "  )) AS recebedor_segundos_desde_ult_tx,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 4. FEATURES DE CONTEXTO DA CONTA (Sinais de \"Laranja\")\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: coalesce(..., 0) para contas que não forem encontradas no JOIN\n",
    "  coalesce(DATEDIFF(date(tx.data), date(conta_orig.aberta_em)), 0) AS pagador_dias_de_conta,\n",
    "  coalesce(DATEDIFF(date(tx.data), date(conta_dest.aberta_em)), 0) AS recebedor_dias_de_conta,\n",
    "\n",
    "  -- Mitigação: coalesce(..., 0) + 0.01 para evitar NULLs e divisão por zero\n",
    "  (\n",
    "    tx.valor / (coalesce(conta_orig.saldo, 0) + 0.01)\n",
    "  ) AS tx_vs_saldo_pagador_ratio,\n",
    "  \n",
    "  -- Mitigação: coalesce(..., -1) para idade desconhecida (ex: cliente não encontrado)\n",
    "  coalesce(\n",
    "    floor(DATEDIFF(date(tx.data), date(cliente_orig.nascido_em)) / 365.25),\n",
    "    -1\n",
    "  ) AS pagador_idade_cliente_anos,\n",
    "  \n",
    "  coalesce(\n",
    "    floor(DATEDIFF(date(tx.data), date(cliente_dest.nascido_em)) / 365.25),\n",
    "    -1\n",
    "  ) AS recebedor_idade_cliente_anos,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 5. FEATURES DE CHAVE PIX (Simplificadas pela CTE)\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: A CTE já usa coalesce(..., 0)\n",
    "  conta_dest.qtd_chaves AS recebedor_qtd_chaves_pix,\n",
    "\n",
    "  -- Mitigação: coalesce(..., 9999) para \"tempo muito longo / nunca cadastrou\"\n",
    "  coalesce(\n",
    "    DATEDIFF(date(tx.data), date(conta_dest.ultima_chave_em)),\n",
    "    9999\n",
    "  ) AS recebedor_dias_desde_ult_chave_pix,\n",
    "  \n",
    "  -- \"Sinal de Laranja\": Dias entre abertura da conta e 1ª chave\n",
    "  -- Mitigação: coalesce(..., 9999) para \"tempo muito longo / nunca cadastrou\"\n",
    "  coalesce(\n",
    "    DATEDIFF(date(conta_dest.primeira_chave_em), date(conta_dest.aberta_em)),\n",
    "    9999\n",
    "  ) AS recebedor_dias_abertura_ate_1a_chave,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 6. FEATURES DE TRIANGULAÇÃO (Diversidade de Contas)\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: Nenhuma necessária. Esta lógica é robusta.\n",
    "  size(array_distinct(collect_list(tx.id_conta_origem) OVER (\n",
    "      PARTITION BY tx.id_conta_destino, date(tx.data)\n",
    "  ))) AS recebedor_pagadores_unicos_dia,\n",
    "\n",
    "  size(array_distinct(collect_list(tx.id_conta_destino) OVER (\n",
    "      PARTITION BY tx.id_conta_origem, date(tx.data)\n",
    "  ))) AS pagador_recebedores_unicos_dia,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 7. FEATURES DE CONTAGEM E HISTÓRICO\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: Nenhuma necessária. Esta lógica é robusta.\n",
    "  (ROW_NUMBER() OVER (\n",
    "      PARTITION BY tx.id_conta_origem, tx.id_conta_destino\n",
    "      ORDER BY tx.data ASC \n",
    "    ) - 1) as qtd_transacoes_hist_pagador_recebedor, \n",
    "\n",
    "  COUNT(1) OVER (\n",
    "      PARTITION BY tx.id_conta_origem, date(tx.data)\n",
    "  ) as qtd_transacoes_dia_pagador,\n",
    "  \n",
    "  COUNT(1) OVER (\n",
    "      PARTITION BY tx.id_conta_destino, date(tx.data)\n",
    "  ) AS recebedor_total_txs_no_dia,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 8. FEATURES ESTATÍSTICAS (Detecção de Anomalia)\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: coalesce(..., 0) para a 1ª transação (histórico nulo)\n",
    "  coalesce(\n",
    "    AVG(tx.valor) OVER (\n",
    "        PARTITION BY tx.id_conta_origem ORDER BY tx.data ASC\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n",
    "    ), 0\n",
    "  ) AS pagador_media_valor_hist,\n",
    "\n",
    "  -- Mitigação: coalesce(..., 0) para a 1ª/2ª transação (histórico nulo)\n",
    "  coalesce(\n",
    "    STDDEV(tx.valor) OVER (\n",
    "        PARTITION BY tx.id_conta_origem ORDER BY tx.data ASC\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n",
    "    ), 0\n",
    "  ) AS pagador_stddev_valor_hist,\n",
    "\n",
    "  -- ===================================================================\n",
    "  -- 9. FEATURES CATEGÓRICAS (IDs brutos)\n",
    "  -- ===================================================================\n",
    "  -- Mitigação: coalesce(..., -1) para IDs desconhecidos\n",
    "  coalesce(conta_orig.id_tipo_conta, -1) AS pagador_tipo_conta_id,\n",
    "  coalesce(cliente_orig.id_natureza, -1) AS pagador_natureza_id,\n",
    "  coalesce(conta_dest.id_tipo_conta, -1) AS recebedor_tipo_conta_id,\n",
    "  coalesce(cliente_dest.id_natureza, -1) AS recebedor_natureza_id\n",
    "\n",
    "FROM\n",
    "  transacoes_db.copper.transacoes AS tx\n",
    "\n",
    "-- Joins do Pagador (Usando a CTE)\n",
    " LEFT JOIN contas_enriquecidas AS conta_orig\n",
    "  ON tx.id_conta_origem = conta_orig.id\n",
    " LEFT JOIN transacoes_db.copper.clientes AS cliente_orig\n",
    "  ON conta_orig.id_cliente = cliente_orig.id\n",
    "\n",
    "-- Joins do Recebedor (Usando a CTE)\n",
    " LEFT JOIN contas_enriquecidas AS conta_dest\n",
    "  ON tx.id_conta_destino = conta_dest.id\n",
    " LEFT JOIN transacoes_db.copper.clientes AS cliente_dest\n",
    "  ON conta_dest.id_cliente = cliente_dest.id\n",
    "  \n",
    "-- Outros Joins Dimensionais (Opcional)\n",
    " LEFT JOIN transacoes_db.copper.finalidade_pix AS finalidade_pix\n",
    "  ON tx.id_finalidade_pix = finalidade_pix.id\n",
    "\n",
    "ORDER BY\n",
    "  tx.data ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "712ca9fa-5021-46f8-a07b-ec9030f1323a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- nulos\n",
    "SELECT\n",
    "  -- Contagem de segurança para Nulos\n",
    "  SUM(CASE WHEN transacao_fraudulenta IS NULL THEN 1 ELSE 0 END) AS nulos_transacao_fraudulenta,\n",
    "  SUM(CASE WHEN valor_transacao IS NULL THEN 1 ELSE 0 END) AS nulos_valor_transacao,\n",
    "  SUM(CASE WHEN data_transacao IS NULL THEN 1 ELSE 0 END) AS nulos_data_transacao,\n",
    "  \n",
    "  -- Features de Velocidade\n",
    "  SUM(CASE WHEN pagador_segundos_desde_ult_tx IS NULL THEN 1 ELSE 0 END) AS nulos_pagador_segundos_desde_ult_tx,\n",
    "  SUM(CASE WHEN recebedor_segundos_desde_ult_tx IS NULL THEN 1 ELSE 0 END) AS nulos_recebedor_segundos_desde_ult_tx,\n",
    "  \n",
    "  -- Features de Contexto da Conta\n",
    "  SUM(CASE WHEN pagador_dias_de_conta IS NULL THEN 1 ELSE 0 END) AS nulos_pagador_dias_de_conta,\n",
    "  SUM(CASE WHEN tx_vs_saldo_pagador_ratio IS NULL THEN 1 ELSE 0 END) AS nulos_tx_vs_saldo_pagador_ratio,\n",
    "  SUM(CASE WHEN pagador_idade_cliente_anos IS NULL THEN 1 ELSE 0 END) AS nulos_pagador_idade_cliente_anos,\n",
    "  \n",
    "  -- Features de Chave Pix\n",
    "  SUM(CASE WHEN recebedor_qtd_chaves_pix IS NULL THEN 1 ELSE 0 END) AS nulos_recebedor_qtd_chaves_pix,\n",
    "  SUM(CASE WHEN recebedor_dias_desde_ult_chave_pix IS NULL THEN 1 ELSE 0 END) AS nulos_recebedor_dias_desde_ult_chave_pix,\n",
    "  SUM(CASE WHEN recebedor_dias_abertura_ate_1a_chave IS NULL THEN 1 ELSE 0 END) AS nulos_recebedor_dias_abertura_ate_1a_chave,\n",
    "  \n",
    "  -- Features Estatísticas\n",
    "  SUM(CASE WHEN pagador_media_valor_hist IS NULL THEN 1 ELSE 0 END) AS nulos_pagador_media_valor_hist,\n",
    "  SUM(CASE WHEN pagador_stddev_valor_hist IS NULL THEN 1 ELSE 0 END) AS nulos_pagador_stddev_valor_hist,\n",
    "  \n",
    "  -- Features Categóricas\n",
    "  SUM(CASE WHEN pagador_tipo_conta_id IS NULL THEN 1 ELSE 0 END) AS nulos_pagador_tipo_conta_id,\n",
    "  SUM(CASE WHEN recebedor_natureza_id IS NULL THEN 1 ELSE 0 END) AS nulos_recebedor_natureza_id\n",
    "FROM\n",
    "  transacoes_db.gold.feature_store_fraude;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3dc604dd-a702-4fc7-b32f-7e6f93968022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- cardinalidade\n",
    "SELECT\n",
    "  COUNT(DISTINCT tipo_iniciacao_pix_id) AS cardinalidade_iniciacao_pix,\n",
    "  COUNT(DISTINCT finalidade_pix_id) AS cardinalidade_finalidade_pix,\n",
    "  COUNT(DISTINCT pagador_tipo_conta_id) AS cardinalidade_pagador_tipo_conta,\n",
    "  COUNT(DISTINCT pagador_natureza_id) AS cardinalidade_pagador_natureza,\n",
    "  COUNT(DISTINCT recebedor_tipo_conta_id) AS cardinalidade_recebedor_tipo_conta,\n",
    "  COUNT(DISTINCT recebedor_natureza_id) AS cardinalidade_recebedor_natureza,\n",
    "  COUNT(DISTINCT hora_do_dia) AS cardinalidade_hora_do_dia\n",
    "FROM\n",
    "  transacoes_db.gold.feature_store_fraude;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4900604027270253,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "gerador_de_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
