{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892a9927-712c-430a-a438-9c50588fde5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade scikit-learn==1.4.2 tensorflow==2.16.1 scikeras==0.13.0 joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8bd723d-0015-4a1a-a5f3-eed4664a619d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43841998-8382-4b63-903e-df2380a71cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "#  pix_fraud_inference_v10.py\n",
    "#  Versão completa — inclui probabilidades por tipo de fraude\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Configurações de caminhos e artefatos\n",
    "# ==========================================================\n",
    "ARTIFACT_DIR = \"/Volumes/transacoes_db/copper/files/pix_fraud/production_artifacts\"\n",
    "\n",
    "BINARY_MODEL_PATH = os.path.join(ARTIFACT_DIR, \"fraud_binary_pipeline\")\n",
    "MULTICLASS_MODEL_PATH = os.path.join(ARTIFACT_DIR, \"fraud_type_pipeline\")\n",
    "LABEL_MAP_PATH = os.path.join(ARTIFACT_DIR, \"fraud_type_label_map.json\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Carregamento dos modelos e metadados\n",
    "# ==========================================================\n",
    "print(f\"Carregando modelo binário de: {BINARY_MODEL_PATH}...\")\n",
    "model_binary = mlflow.sklearn.load_model(BINARY_MODEL_PATH)\n",
    "print(\"Modelo binário carregado com sucesso.\")\n",
    "\n",
    "print(f\"Carregando modelo multiclasse de: {MULTICLASS_MODEL_PATH}...\")\n",
    "model_multiclass = mlflow.sklearn.load_model(MULTICLASS_MODEL_PATH)\n",
    "print(\"Modelo multiclasse carregado com sucesso.\")\n",
    "\n",
    "print(f\"Carregando mapa de labels de: {LABEL_MAP_PATH}...\")\n",
    "with open(LABEL_MAP_PATH, \"r\") as f:\n",
    "    label_map_raw = json.load(f)\n",
    "\n",
    "# Normalizar o mapa de labels para int -> nome e criar lista ordenada de classes\n",
    "label_map_int = {int(k): v for k, v in label_map_raw.items()}\n",
    "multiclass_class_names = [label_map_int[k] for k in sorted(label_map_int.keys())]\n",
    "print(f\"Mapa de labels carregado. Classes ordenadas: {multiclass_class_names}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Função de inferência\n",
    "# ==========================================================\n",
    "def run_inference(input_df: pd.DataFrame, fraud_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Executa a inferência completa:\n",
    "    1. Modelo binário → fraude ou legítima\n",
    "    2. Modelo multiclasse → tipo de fraude (se aplicável)\n",
    "    Retorna dict com probabilidades e mapeamentos por tipo.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Recebida {len(input_df)} transação(s) para análise. ---\")\n",
    "    print(\"DEBUG - input_df.dtypes:\")\n",
    "    print(input_df.dtypes)\n",
    "    print(\"DEBUG - input_df.head():\")\n",
    "    print(input_df.head().to_string())\n",
    "\n",
    "    # ======================================================\n",
    "    # Etapa 1: Modelo Binário (Detecção de Fraude)\n",
    "    # ======================================================\n",
    "    print(\"\\nExecutando modelo binário...\")\n",
    "    pred_binary_proba_array = model_binary.predict_proba(input_df)\n",
    "    print(f\"DEBUG - Probabilidades retornadas pelo modelo binário: {pred_binary_proba_array}\")\n",
    "\n",
    "    # --- CORREÇÃO ROBUSTA PARA VÁRIOS FORMATOS DE SAÍDA ---\n",
    "    proba = pred_binary_proba_array\n",
    "\n",
    "    if isinstance(proba, np.ndarray) and proba.ndim == 2:\n",
    "        # formato esperado: [[p0, p1]]\n",
    "        pred_binary_proba_float = float(round(proba[0][1], 4))\n",
    "    elif isinstance(proba, np.ndarray) and proba.ndim == 1 and len(proba) == 2:\n",
    "        # formato: [p0, p1]\n",
    "        pred_binary_proba_float = float(round(proba[1], 4))\n",
    "    elif isinstance(proba, np.ndarray) and proba.ndim == 1 and len(proba) == 1:\n",
    "        # formato: [p1] - probabilidade direta\n",
    "        pred_binary_proba_float = float(round(proba[0], 4))\n",
    "    else:\n",
    "        # Pode ser float ou outro tipo; forçar float\n",
    "        pred_binary_proba_float = float(round(float(proba), 4))\n",
    "\n",
    "    print(f\"DEBUG - Probabilidade normalizada (fraude=1): {pred_binary_proba_float}\")\n",
    "\n",
    "    pred_binary_label = int(pred_binary_proba_float >= fraud_threshold)\n",
    "    print(f\"Probabilidade de fraude (classe=1): {pred_binary_proba_float}\")\n",
    "    print(f\"Predição binária final: {'FRAUDE' if pred_binary_label == 1 else 'LEGÍTIMA'}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Etapa 2: Modelo Multiclasse (Classificação do Tipo de Fraude)\n",
    "    # ======================================================\n",
    "    pred_multi_label = None\n",
    "    pred_multi_proba = None\n",
    "    pred_multi_proba_dict = None\n",
    "\n",
    "    if pred_binary_label == 1:\n",
    "        print(\"\\nExecutando modelo multiclasse (tipo de fraude)...\")\n",
    "        pred_multi_proba_array = model_multiclass.predict_proba(input_df)\n",
    "        print(f\"DEBUG - Probabilidades retornadas pelo modelo multiclasse: {pred_multi_proba_array}\")\n",
    "\n",
    "        # Normalizar para vetor de probabilidades por classes\n",
    "        if isinstance(pred_multi_proba_array, np.ndarray) and pred_multi_proba_array.ndim == 2:\n",
    "            prob_array = pred_multi_proba_array[0]\n",
    "        elif isinstance(pred_multi_proba_array, np.ndarray) and pred_multi_proba_array.ndim == 1:\n",
    "            prob_array = pred_multi_proba_array\n",
    "        else:\n",
    "            prob_array = np.array(pred_multi_proba_array).ravel()\n",
    "\n",
    "        # Caso o número de probabilidades não bata com o número de classes,\n",
    "        # cortamos/expandimos com zeros (defensivo).\n",
    "        n_classes = len(multiclass_class_names)\n",
    "        if prob_array.size != n_classes:\n",
    "            print(f\"Aviso: número de probabilidades ({prob_array.size}) != número de classes ({n_classes}). Ajustando de forma defensiva.\")\n",
    "            # Ajustar: se houver mais probs, truncar; se houver menos, preencher com zeros\n",
    "            if prob_array.size > n_classes:\n",
    "                prob_array = prob_array[:n_classes]\n",
    "            else:\n",
    "                prob_array = np.concatenate([prob_array, np.zeros(n_classes - prob_array.size)])\n",
    "\n",
    "        # Mapear nome_da_classe -> probabilidade\n",
    "        pred_multi_proba_dict = {\n",
    "            multiclass_class_names[i]: float(round(float(prob_array[i]), 4))\n",
    "            for i in range(len(multiclass_class_names))\n",
    "        }\n",
    "\n",
    "        # Escolher label e confiança\n",
    "        argmax_idx = int(np.argmax(prob_array))\n",
    "        pred_multi_label = multiclass_class_names[argmax_idx]\n",
    "        pred_multi_proba = float(round(float(prob_array[argmax_idx]), 4))\n",
    "\n",
    "        print(f\"Tipo de fraude previsto: {pred_multi_label} (confiança={pred_multi_proba})\")\n",
    "        print(f\"Probabilidades por tipo: {pred_multi_proba_dict}\")\n",
    "    else:\n",
    "        print(\"Transação legítima — modelo multiclasse não executado.\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Retornar resultado consolidado\n",
    "    # ======================================================\n",
    "    result = {\n",
    "        \"is_fraud\": bool(pred_binary_label),\n",
    "        \"fraud_probability\": pred_binary_proba_float,\n",
    "        \"fraud_type\": pred_multi_label,\n",
    "        \"fraud_type_confidence\": pred_multi_proba,\n",
    "        \"fraud_type_probabilities\": pred_multi_proba_dict,\n",
    "        \"timestamp_inferencia\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# ==========================================================\n",
    "# 4. Exemplo de teste\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Simular entrada de uma transação (formato: 1 linha)\n",
    "    \n",
    "    data = {\n",
    "        \"id_transacao\": [\"t2-teste-real-fraude\"],\n",
    "        \"pagador_conta_aberta_em\": [\"2023-05-10T10:00:00\"],\n",
    "        \"pagador_segundos_desde_ultima_tx\": [300],\n",
    "        \"pagador_data_nascimento\": [\"1999-07-15T00:00:00\"],\n",
    "        \"valor_transacao\": [1200000.0],\n",
    "        \"tipo_iniciacao_pix_id\": [2],\n",
    "        \"recebedor_txs_ultima_1h\": [5],\n",
    "        \"recebedor_idade_conta_dias\": [10],\n",
    "        \"pagador_tipo_conta_id\": [2],\n",
    "        \"pagador_valor_ultimas_24h\": [19500.0],\n",
    "        \"pagador_interacoes_com_recebedor\": [1],\n",
    "        \"finalidade_pix_id\": [3],\n",
    "        \"recebedor_natureza_id\": [1],\n",
    "        \"recebedor_valor_ultima_1h\": [19500.0],\n",
    "        \"recebedor_saldo\": [19500.0],\n",
    "        \"recebedor_tipo_conta_id\": [2],\n",
    "        \"pagador_idade_conta_dias\": [90],\n",
    "        \"primeira_interacao\": [1],\n",
    "        \"valor_vs_saldo_pagador\": [0.9],\n",
    "        \"recebedor_data_nascimento\": [\"2001-01-01T00:00:00\"],\n",
    "        \"pagador_saldo\": [20000.0],\n",
    "        \"pagador_txs_ultimas_24h\": [1],\n",
    "        \"recebedor_num_pagadores_unicos_24h\": [1],\n",
    "        \"recebedor_conta_aberta_em\": [\"2025-11-01T00:00:00\"],\n",
    "        \"pagador_natureza_id\": [1],\n",
    "        \"data_transacao\": [\"2025-11-08T23:30:00\"],\n",
    "        \"valor_vs_media_pagador_30d\": [25.8],\n",
    "    }\n",
    "\n",
    "    df_input = pd.DataFrame(data)\n",
    "    resultado = run_inference(df_input)\n",
    "\n",
    "    print(\"\\n--- Resultado Final ---\")\n",
    "    print(json.dumps(resultado, indent=4, ensure_ascii=False))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8e64d0c5-4f7f-41d1-aec3-c70f0d7bc4f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import azure.functions as func\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Configurações de Artefatos (Lendo do Blob)\n",
    "# ==========================================================\n",
    "\n",
    "# !! IMPORTANTE !!\n",
    "# Certifique-se de que estas variáveis de ambiente estão definidas\n",
    "# nas \"Configurações\" do seu Aplicativo de Funções no Portal do Azure.\n",
    "CONNECTION_STRING = os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"]\n",
    "CONTAINER_NAME = \"modelos-fraude\" # O nome do contêiner que você criou\n",
    "\n",
    "# O Azure Functions fornece o diretório /tmp como um disco temporário\n",
    "LOCAL_ARTIFACT_DIR = \"/tmp/pix_fraud_artifacts\"\n",
    "if not os.path.exists(LOCAL_ARTIFACT_DIR):\n",
    "    os.makedirs(LOCAL_ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# Caminhos locais para onde os modelos serão baixados\n",
    "BINARY_MODEL_PATH = os.path.join(LOCAL_ARTIFACT_DIR, \"fraud_binary_pipeline\")\n",
    "MULTICLASS_MODEL_PATH = os.path.join(LOCAL_ARTIFACT_DIR, \"fraud_type_pipeline\")\n",
    "LABEL_MAP_PATH = os.path.join(LOCAL_ARTIFACT_DIR, \"fraud_type_label_map.json\")\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Funções Auxiliares para Download do Blob\n",
    "# ==========================================================\n",
    "\n",
    "def download_blob_file(blob_service_client, container_name, remote_file, local_file):\n",
    "    \"\"\" Baixa um único arquivo do blob. \"\"\"\n",
    "    try:\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=remote_file)\n",
    "        # Cria o diretório local se não existir\n",
    "        os.makedirs(os.path.dirname(local_file), exist_ok=True)\n",
    "        with open(local_file, \"wb\") as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "        logging.info(f\"Blob {remote_file} baixado para {local_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Falha ao baixar arquivo {remote_file}: {e}\")\n",
    "        raise\n",
    "\n",
    "def download_blob_directory(blob_service_client, container_name, remote_dir, local_dir):\n",
    "    \"\"\" Baixa um 'diretório' (prefixo) do blob. \"\"\"\n",
    "    try:\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "        blob_list = container_client.list_blobs(name_starts_with=remote_dir)\n",
    "        \n",
    "        for blob in blob_list:\n",
    "            # Define o caminho local completo\n",
    "            relative_path = os.path.relpath(blob.name, remote_dir)\n",
    "            local_file_path = os.path.join(local_dir, relative_path)\n",
    "            \n",
    "            # Cria subdiretórios se não existirem\n",
    "            local_file_dir = os.path.dirname(local_file_path)\n",
    "            if not os.path.exists(local_file_dir):\n",
    "                os.makedirs(local_file_dir, exist_ok=True)\n",
    "                \n",
    "            # Baixa o arquivo\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            with open(local_file_path, \"wb\") as download_file:\n",
    "                download_file.write(blob_client.download_blob().readall())\n",
    "        \n",
    "        logging.info(f\"Diretório blob {remote_dir} baixado para {local_dir}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Falha ao baixar diretório {remote_dir}: {e}\")\n",
    "        raise\n",
    "\n",
    "# ==========================================================\n",
    "# 3. Carregamento Global dos Modelos (Executado no Cold Start)\n",
    "# ==========================================================\n",
    "logging.info(\"Iniciando o carregamento global dos modelos (cold start)...\")\n",
    "try:\n",
    "    # Conectar ao Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING)\n",
    "    \n",
    "    # --- Baixar Artefatos do Blob ---\n",
    "    logging.info(f\"Baixando modelo binário de {CONTAINER_NAME}/fraud_binary_pipeline...\")\n",
    "    download_blob_directory(blob_service_client, CONTAINER_NAME, \"fraud_binary_pipeline\", BINARY_MODEL_PATH)\n",
    "    \n",
    "    logging.info(f\"Baixando modelo multiclasse de {CONTAINER_NAME}/fraud_type_pipeline...\")\n",
    "    download_blob_directory(blob_service_client, CONTAINER_NAME, \"fraud_type_pipeline\", MULTICLASS_MODEL_PATH)\n",
    "    \n",
    "    logging.info(f\"Baixando mapa de labels de {CONTAINER_NAME}/fraud_type_label_map.json...\")\n",
    "    download_blob_file(blob_service_client, CONTAINER_NAME, \"fraud_type_label_map.json\", LABEL_MAP_PATH)\n",
    "    \n",
    "    logging.info(\"Artefatos baixados. Carregando modelos do disco local (/tmp)...\")\n",
    "\n",
    "    # --- Carregar Modelos na Memória ---\n",
    "    model_binary = mlflow.sklearn.load_model(BINARY_MODEL_PATH)\n",
    "    model_multiclass = mlflow.sklearn.load_model(MULTICLASS_MODEL_PATH)\n",
    "    \n",
    "    with open(LABEL_MAP_PATH, \"r\") as f:\n",
    "        label_map_raw = json.load(f)\n",
    "    \n",
    "    label_map_int = {int(k): v for k, v in label_map_raw.items()}\n",
    "    multiclass_class_names = [label_map_int[k] for k in sorted(label_map_int.keys())]\n",
    "    \n",
    "    logging.info(f\"Modelos carregados com sucesso. Classes: {multiclass_class_names}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Se falhar aqui, a função não funcionará.\n",
    "    logging.critical(f\"FALHA CRÍTICA NO COLD START: Não foi possível carregar os modelos. Erro: {e}\")\n",
    "    model_binary = None \n",
    "    model_multiclass = None\n",
    "    multiclass_class_names = []\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4. Função de Inferência (Seu código, com logging)\n",
    "# ==========================================================\n",
    "def run_inference(input_df: pd.DataFrame, fraud_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Executa a inferência completa.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n--- Recebida {len(input_df)} transação(s) para análise. ---\")\n",
    "    logging.info(\"DEBUG - input_df.dtypes:\")\n",
    "    logging.info(input_df.dtypes)\n",
    "\n",
    "    # ======================================================\n",
    "    # Etapa 1: Modelo Binário (Detecção de Fraude)\n",
    "    # ======================================================\n",
    "    logging.info(\"\\nExecutando modelo binário...\")\n",
    "    pred_binary_proba_array = model_binary.predict_proba(input_df)\n",
    "    logging.info(f\"DEBUG - Probabilidades retornadas pelo modelo binário: {pred_binary_proba_array}\")\n",
    "\n",
    "    # --- CORREÇÃO ROBUSTA PARA VÁRIOS FORMATOS DE SAÍDA ---\n",
    "    proba = pred_binary_proba_array\n",
    "    if isinstance(proba, np.ndarray) and proba.ndim == 2:\n",
    "        pred_binary_proba_float = float(round(proba[0][1], 4))\n",
    "    elif isinstance(proba, np.ndarray) and proba.ndim == 1 and len(proba) == 2:\n",
    "        pred_binary_proba_float = float(round(proba[1], 4))\n",
    "    elif isinstance(proba, np.ndarray) and proba.ndim == 1 and len(proba) == 1:\n",
    "        pred_binary_proba_float = float(round(proba[0], 4))\n",
    "    else:\n",
    "        pred_binary_proba_float = float(round(float(proba), 4))\n",
    "\n",
    "    logging.info(f\"DEBUG - Probabilidade normalizada (fraude=1): {pred_binary_proba_float}\")\n",
    "\n",
    "    pred_binary_label = int(pred_binary_proba_float >= fraud_threshold)\n",
    "    logging.info(f\"Probabilidade de fraude (classe=1): {pred_binary_proba_float}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Etapa 2: Modelo Multiclasse (Classificação do Tipo de Fraude)\n",
    "    # ======================================================\n",
    "    pred_multi_label = None\n",
    "    pred_multi_proba = None\n",
    "    pred_multi_proba_dict = None\n",
    "\n",
    "    if pred_binary_label == 1:\n",
    "        logging.info(\"\\nExecutando modelo multiclasse (tipo de fraude)...\")\n",
    "        pred_multi_proba_array = model_multiclass.predict_proba(input_df)\n",
    "        logging.info(f\"DEBUG - Probabilidades retornadas pelo modelo multiclasse: {pred_multi_proba_array}\")\n",
    "\n",
    "        # Normalizar para vetor de probabilidades por classes\n",
    "        if isinstance(pred_multi_proba_array, np.ndarray) and pred_multi_proba_array.ndim == 2:\n",
    "            prob_array = pred_multi_proba_array[0]\n",
    "        elif isinstance(pred_multi_proba_array, np.ndarray) and pred_multi_proba_array.ndim == 1:\n",
    "            prob_array = pred_multi_proba_array\n",
    "        else:\n",
    "            prob_array = np.array(pred_multi_proba_array).ravel()\n",
    "\n",
    "        # Defensivo\n",
    "        n_classes = len(multiclass_class_names)\n",
    "        if prob_array.size != n_classes:\n",
    "            logging.warning(f\"Aviso: número de probabilidades ({prob_array.size}) != número de classes ({n_classes}). Ajustando.\")\n",
    "            if prob_array.size > n_classes:\n",
    "                prob_array = prob_array[:n_classes]\n",
    "            else:\n",
    "                prob_array = np.concatenate([prob_array, np.zeros(n_classes - prob_array.size)])\n",
    "\n",
    "        # Mapear nome_da_classe -> probabilidade\n",
    "        pred_multi_proba_dict = {\n",
    "            multiclass_class_names[i]: float(round(float(prob_array[i]), 4))\n",
    "            for i in range(len(multiclass_class_names))\n",
    "        }\n",
    "\n",
    "        # Escolher label e confiança\n",
    "        argmax_idx = int(np.argmax(prob_array))\n",
    "        pred_multi_label = multiclass_class_names[argmax_idx]\n",
    "        pred_multi_proba = float(round(float(prob_array[argmax_idx]), 4))\n",
    "        logging.info(f\"Probabilidades por tipo: {pred_multi_proba_dict}\")\n",
    "    else:\n",
    "        logging.info(\"Transação legítima — modelo multiclasse não executado.\")\n",
    "\n",
    "    # ======================================================\n",
    "    # Retornar resultado consolidado\n",
    "    # ======================================================\n",
    "    result = {\n",
    "        \"is_fraud\": bool(pred_binary_label),\n",
    "        \"fraud_probability\": pred_binary_proba_float,\n",
    "        \"fraud_type\": pred_multi_label,\n",
    "        \"fraud_type_confidence\": pred_multi_proba,\n",
    "        \"fraud_type_probabilities\": pred_multi_proba_dict,\n",
    "        \"timestamp_inferencia\": datetime.now().isoformat()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5. Ponto de Entrada da Azure Function (O gatilho HTTP)\n",
    "# ==========================================================\n",
    "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
    "    logging.info('Requisição HTTP recebida para inferência de fraude.')\n",
    "\n",
    "    # Verificar se os modelos foram carregados corretamente no cold start\n",
    "    if model_binary is None or model_multiclass is None:\n",
    "        logging.error(\"Modelos não estão carregados. Verifique o log de cold start.\")\n",
    "        return func.HttpResponse(\n",
    "             \"Erro interno: Modelos não puderam ser carregados.\",\n",
    "             status_code=500\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Receber o JSON do corpo da requisição\n",
    "        req_body = req.get_json()\n",
    "        \n",
    "        # Lógica para garantir que os dados estejam no formato de lista\n",
    "        # que o pd.DataFrame espera (para uma única linha)\n",
    "        data_for_df = {}\n",
    "        for key, value in req_body.items():\n",
    "            if not isinstance(value, list):\n",
    "                data_for_df[key] = [value]\n",
    "            else:\n",
    "                data_for_df[key] = value\n",
    "\n",
    "        df_input = pd.DataFrame(data_for_df)\n",
    "\n",
    "        # Chamar a função de inferência\n",
    "        resultado = run_inference(df_input)\n",
    "\n",
    "        # Retornar o JSON\n",
    "        return func.HttpResponse(\n",
    "            json.dumps(resultado, indent=4, ensure_ascii=False),\n",
    "            status_code=200,\n",
    "            mimetype=\"application/json\"\n",
    "        )\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(f\"Erro de valor ou JSON mal formatado: {ve}\")\n",
    "        return func.HttpResponse(f\"JSON inválido ou dados incorretos: {ve}\", status_code=400)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro inesperado na execução: {e}\")\n",
    "        return func.HttpResponse(f\"Erro interno no servidor: {e}\", status_code=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29205c0a-fdd2-4a09-9745-00e9d3f58135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 1. Definição dos Caminhos ---\n",
    "\n",
    "# Origem (no Volume)\n",
    "source_folder = '/Volumes/transacoes_db/copper/files/pix_fraud/production_artifacts'\n",
    "\n",
    "# Destino Final (no Volume)\n",
    "destination_folder = '/Volumes/transacoes_db/copper/files/'\n",
    "final_zip_name = 'production_artifacts.zip'\n",
    "final_zip_path = os.path.join(destination_folder, final_zip_name)\n",
    "\n",
    "# Caminhos Temporários (no disco local do cluster)\n",
    "local_temp_copy = '/tmp/artifacts_copy'           # Para onde os arquivos serão copiados\n",
    "local_temp_zip_base = '/tmp/production_artifacts' # Onde o zip será criado\n",
    "local_zip_file_path = f\"{local_temp_zip_base}.zip\" # O nome do zip local\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # --- Limpeza de execuções anteriores ---\n",
    "    if os.path.exists(local_temp_copy):\n",
    "        print(f\"Limpando pasta temporária antiga: {local_temp_copy}\")\n",
    "        shutil.rmtree(local_temp_copy)\n",
    "    if os.path.exists(local_zip_file_path):\n",
    "        print(f\"Limpando zip temporário antigo: {local_zip_file_path}\")\n",
    "        os.remove(local_zip_file_path)\n",
    "    \n",
    "    # --- Etapa 1: Copiar do Volume para o Disco Local ---\n",
    "    print(f\"Copiando {source_folder} para {local_temp_copy}...\")\n",
    "    shutil.copytree(source_folder, local_temp_copy)\n",
    "    print(\"Cópia local concluída.\")\n",
    "    \n",
    "    # --- Etapa 2: Compactar no Disco Local (Local -> Local) ---\n",
    "    print(f\"Compactando {local_temp_copy} para {local_zip_file_path}...\")\n",
    "    shutil.make_archive(\n",
    "        base_name=local_temp_zip_base, # Onde salvar em /tmp (sem .zip)\n",
    "        format='zip',\n",
    "        root_dir='/tmp',\n",
    "        base_dir='artifacts_copy'      # O que zipar dentro de /tmp\n",
    "    )\n",
    "    print(\"Compactação local concluída.\")\n",
    "    \n",
    "    # --- Etapa 3: Mover o .zip pronto do Disco Local para o Volume ---\n",
    "    print(f\"Movendo {local_zip_file_path} para {final_zip_path}...\")\n",
    "    shutil.move(local_zip_file_path, final_zip_path)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"\\nSucesso! Arquivo final movido para:\")\n",
    "    print(final_zip_path)\n",
    "    print(f\"Tempo de execução: {duration:.2f} segundos.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro: {e}\")\n",
    "\n",
    "finally:\n",
    "    # --- Limpeza Final ---\n",
    "    if os.path.exists(local_temp_copy):\n",
    "        print(f\"Limpando pasta temporária final: {local_temp_copy}...\")\n",
    "        shutil.rmtree(local_temp_copy)\n",
    "    if os.path.exists(local_zip_file_path):\n",
    "        # Limpa o zip local caso o 'move' falhe\n",
    "        print(f\"Limpando arquivo zip temporário: {local_zip_file_path}...\")\n",
    "        os.remove(local_zip_file_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Testes e Aplicacoes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
