{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6783814a-a7bb-485b-a5d3-26e65edb4c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Definição de Caminhos ---\n",
    "nome_arquivo = \"estados.csv\"\n",
    "caminho_final_arquivo = f\"/Volumes/transacoes_db/copper/files/{nome_arquivo}\"\n",
    "caminho_temporario_spark = f\"/Volumes/transacoes_db/copper/files/temp_chaves_pix_save\"\n",
    "\n",
    "# --- 2. Sua Query ---\n",
    "query = \"\"\"\n",
    "select \n",
    "codigo_uf as id, nome, id_regiao, uf as sigla\n",
    "from transacoes_db.copper.estados\n",
    "\"\"\"\n",
    "\n",
    "resultado_df = spark.sql(query)\n",
    "count_query = resultado_df.count()\n",
    "print(f\"Quantidade de linhas na query: {count_query}\")\n",
    "\n",
    "# --- 3. Salvar em Arquivo Único (com .coalesce) ---\n",
    "print(f\"Salvando dados em diretório temporário: {caminho_temporario_spark}...\")\n",
    "resultado_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(caminho_temporario_spark)\n",
    "print(\"Dados salvos temporariamente.\")\n",
    "\n",
    "# --- 4. Mover e Renomear o Arquivo (Com Correção) ---\n",
    "try:\n",
    "    # 1. Listar os arquivos no temp\n",
    "    arquivos_no_temp = dbutils.fs.ls(caminho_temporario_spark)\n",
    "\n",
    "    # 2. Encontrar o 'part-'\n",
    "    arquivo_part = [f.path for f in arquivos_no_temp if f.name.startswith(\"part-\") and f.name.endswith(\".csv\")]\n",
    "\n",
    "    if not arquivo_part:\n",
    "        print(\"Erro: Nenhum arquivo 'part-' foi encontrado no diretório temporário.\")\n",
    "    else:\n",
    "        caminho_arquivo_origem = arquivo_part[0] \n",
    "        \n",
    "        # --- !! CORREÇÃO IMPORTANTE !! ---\n",
    "        # Garante que o destino não exista antes de mover.\n",
    "        # Isso impede que o 'mv' mova o arquivo PARA DENTRO de um diretório existente.\n",
    "        print(f\"Limpando destino antigo (se existir): {caminho_final_arquivo}\")\n",
    "        dbutils.fs.rm(caminho_final_arquivo, recurse=True) # Deleta o arquivo OU diretório\n",
    "        \n",
    "        # 3. Mover e renomear o arquivo\n",
    "        print(f\"Movendo {caminho_arquivo_origem} para {caminho_final_arquivo}...\")\n",
    "        # recurse=False é o correto para mover um *arquivo*\n",
    "        dbutils.fs.mv(caminho_arquivo_origem, caminho_final_arquivo, recurse=False)\n",
    "\n",
    "        # 4. Limpar o diretório temporário\n",
    "        print(f\"Limpando diretório temporário: {caminho_temporario_spark}\")\n",
    "        dbutils.fs.rm(caminho_temporario_spark, recurse=True)\n",
    "\n",
    "        print(f\"\\nResultado salvo com sucesso no ARQUIVO: {caminho_final_arquivo}\")\n",
    "\n",
    "        # --- 5. Validação ---\n",
    "        print(f\"\\nLendo arquivo CSV final de: {caminho_final_arquivo}\")\n",
    "        csv_df = spark.read \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .load(caminho_final_arquivo)\n",
    "\n",
    "        count_csv = csv_df.count()\n",
    "        print(f\"Quantidade de linhas no arquivo CSV final: {count_csv}\")\n",
    "        \n",
    "        if count_query == count_csv:\n",
    "            print(\"Sucesso! A contagem de linhas bate com a query.\")\n",
    "        else:\n",
    "            print(\"Atenção! A contagem de linhas está diferente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao mover/renomear o arquivo: {e}\")\n",
    "    print(\"Verifique se o dbutils está disponível neste ambiente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de28abc5-a464-4921-b306-952b3a302099",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762645178259}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select \n",
    "codigo_uf as id, nome, id_regiao, uf as sigla\n",
    "from transacoes_db.copper.estados"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8143920005100724,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-08 20_36_42",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
