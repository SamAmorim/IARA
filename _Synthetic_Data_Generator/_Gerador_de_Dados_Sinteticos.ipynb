{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a008c2eb-7fac-4784-807c-7119a1a84188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5844a3af-048a-4579-8a64-5e7ef630f800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e316d1ba-aa89-4bb7-9493-a8443b000461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c098b61a-0d3b-4791-8acc-95c85081808b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicialização do Ambiente e Esquemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ad0bfd-3b2c-49a8-b619-aa39da66a56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Imports Padrão ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import random\n",
    "import calendar\n",
    "import functools\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Iterator, List\n",
    "from faker import Faker\n",
    "\n",
    "from pyspark.sql import DataFrame, Window, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType, DateType, TimestampType, BooleanType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "Faker.seed(GLOBAL_SEED) # Seed para a biblioteca Faker\n",
    "print(f\"INFO: Sessão Spark importada. Seed global definida: {GLOBAL_SEED}\")\n",
    "\n",
    "\n",
    "ANO_ESTATISTICA = 2024\n",
    "LIMITE_MUNICIPIOS_PROCESSADOS = 30\n",
    "FATOR_ESCALA_VOLUME = 0.0005\n",
    "TX_POR_CLIENTE_ESPERADO = 150 \n",
    "PROBABILIDADE_TRANSACAO_INTERMUNICIPAL = 0.20\n",
    "\n",
    "# Dicionários de configuração \n",
    "config_geracao = {\n",
    "    'PROB_CONTA_ALTO_RISCO': 0.05,\n",
    "    'MAX_DIAS_CADASTRO_CHAVE_RISCO': 7,\n",
    "    'DIAS_CHAVE_CONSIDERADA_RECENTE': 15,\n",
    "    'PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO': 0.60,\n",
    "    'PROBABILIDADE_FRAUDE_CHAVE_RECENTE': 0.40,\n",
    "    'PROBABILIDADE_FRAUDE_BASE': 0.35, \n",
    "    'IDADE_MINIMA_ALVO_ENG_SOCIAL': 55, \n",
    "    'PROBABILIDADE_FRAUDE_ENG_SOCIAL_ALVO': 0.80,\n",
    "    'MULTIPLICADOR_MAGNITUDE_FRAUDE': 30.0,\n",
    "    'PROBABILIDADE_ABAIXO_RADAR': 0.4,\n",
    "    'VALORES_LIMITE_RADAR': [999.90, 499.90, 1999.90],\n",
    "    'PROBABILIDADES_TIPO_FRAUDE': {\n",
    "        \"valor_atipico\": 0.80,\n",
    "        \"engenharia_social\": 0.0,\n",
    "        \"triangulacao_conta_laranja\": 0.10,\n",
    "        \"consolidacao_fundos\": 0.10\n",
    "    },\n",
    "    'MIN_FONTES_CONSOLIDACAO': 10,  \n",
    "    'MAX_FONTES_CONSOLIDACAO': 30,  \n",
    "    'JANELA_SEG_CONSOLIDACAO': 600, \n",
    "    'FANOUT_MIN_PROFUNDIDADE': 2,\n",
    "    'FANOUT_MAX_PROFUNDIDADE': 3,\n",
    "    'FANOUT_MIN_LARGURA': 2,\n",
    "    'FANOUT_MAX_LARGURA': 4,\n",
    "    'PROBABILIDADE_TESTE_CONTA': 0.3,\n",
    "    'PROBABILIDADE_ATAQUE_MADRUGADA': 0.70,\n",
    "}\n",
    "\n",
    "perfis_de_uso_dict = {} \n",
    "LISTA_TIPOS_CONTA_LOCAL = [1, 2, 4] \n",
    "LISTA_ISPBS_LOCAL = [\"12345678\", \"87654321\"] \n",
    "\n",
    "SCHEMA_CLIENTES_UDF = StructType([\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"registro_nacional\", StringType(), True),\n",
    "    StructField(\"nascido_em\", DateType(), True)\n",
    "])\n",
    "SCHEMA_CLIENTES_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"id_natureza\", IntegerType(), True), StructField(\"registro_nacional\", StringType(), True),\n",
    "    StructField(\"nascido_em\", DateType(), True), StructField(\"estado_ibge\", IntegerType(), True),\n",
    "    StructField(\"municipio_ibge\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_CONTAS_UDF = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"saldo\", DoubleType(), True),\n",
    "    StructField(\"aberta_em\", DateType(), True), StructField(\"agencia\", StringType(), True),\n",
    "    StructField(\"numero\", StringType(), True), StructField(\"id_tipo_conta\", IntegerType(), True),\n",
    "    StructField(\"ispb_instituicao\", StringType(), True), StructField(\"id_cliente\", StringType(), True),\n",
    "    StructField(\"is_high_risk\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_CONTAS_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"saldo\", DoubleType(), True),\n",
    "    StructField(\"aberta_em\", DateType(), True), StructField(\"agencia\", StringType(), True),\n",
    "    StructField(\"numero\", StringType(), True), StructField(\"id_tipo_conta\", IntegerType(), True),\n",
    "    StructField(\"ispb_instituicao\", StringType(), True), StructField(\"id_cliente\", StringType(), False), \n",
    "    StructField(\"is_high_risk\", IntegerType(), True), StructField(\"estado_ibge\", IntegerType(), True),\n",
    "    StructField(\"municipio_ibge\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_CHAVES_UDF = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"chave\", StringType(), True),\n",
    "    StructField(\"id_tipo_chave\", IntegerType(), True), StructField(\"cadastrada_em\", DateType(), True),\n",
    "    StructField(\"id_conta\", StringType(), True)\n",
    "])\n",
    "SCHEMA_CHAVES_PIX_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"chave\", StringType(), True),\n",
    "    StructField(\"id_tipo_chave\", IntegerType(), True), StructField(\"cadastrada_em\", DateType(), True),\n",
    "    StructField(\"id_conta\", StringType(), False), \n",
    "    StructField(\"estado_ibge\", IntegerType(), True),\n",
    "    StructField(\"municipio_ibge\", IntegerType(), True)\n",
    "])\n",
    "SCHEMA_PARES = StructType([\n",
    "    StructField(\"id_conta_origem\", StringType(), True),\n",
    "    StructField(\"id_conta_destino\", StringType(), True)\n",
    "])\n",
    "SCHEMA_TRANSACOES_UDF = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"data\", TimestampType(), True), StructField(\"mensagem\", StringType(), True),\n",
    "    StructField(\"id_conta_origem\", StringType(), True), StructField(\"id_conta_destino\", StringType(), True),\n",
    "    StructField(\"id_tipo_iniciacao_pix\", IntegerType(), True), StructField(\"id_finalidade_pix\", IntegerType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True), StructField(\"fraud_type\", StringType(), True),\n",
    "    StructField(\"id_transacao_cadeia_pai\", StringType(), True)\n",
    "])\n",
    "SCHEMA_TRANSACOES_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False), \n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"data\", TimestampType(), True), StructField(\"mensagem\", StringType(), True),\n",
    "    StructField(\"id_conta_origem\", StringType(), True), StructField(\"id_conta_destino\", StringType(), True),\n",
    "    StructField(\"id_tipo_iniciacao_pix\", IntegerType(), True), StructField(\"id_finalidade_pix\", IntegerType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True), StructField(\"fraud_type\", StringType(), True),\n",
    "    StructField(\"id_transacao_cadeia_pai\", StringType(), True),\n",
    "    StructField(\"estado_ibge\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\"INFO: CÉLULA 1 (Refatorada) - Configurações, Seed e Schemas carregados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c97dd4e5-87e6-49cc-804c-5df6e353e680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funções para geração de Agentes (CLientes, Contas e Chaves PIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31bc5ee-a950-4064-8ec2-d688a095b1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@F.pandas_udf(SCHEMA_CLIENTES_UDF)\n",
    "def _gerar_detalhes_cliente_udf(id_natureza: pd.Series) -> pd.DataFrame:\n",
    "    # Faker.seed() já foi chamado na Célula 1, garantindo determinismo aqui\n",
    "    local_fake = Faker('pt_BR')\n",
    "    nomes, registros, nascimentos = [], [], []\n",
    "    for nat in id_natureza:\n",
    "        if nat == 1:\n",
    "            nomes.append(local_fake.name())\n",
    "            registros.append(local_fake.cpf())\n",
    "            nascimentos.append(local_fake.date_of_birth(minimum_age=18, maximum_age=80))\n",
    "        else:\n",
    "            nomes.append(local_fake.company())\n",
    "            registros.append(local_fake.cnpj())\n",
    "            nascimentos.append(local_fake.date_between(start_date='-20y', end_date='-1y'))\n",
    "    return pd.DataFrame({\"nome\": nomes, \"registro_nacional\": registros, \"nascido_em\": nascimentos})\n",
    "\n",
    "def _gerar_clientes(num_pf: int, num_pj: int, estado_ibge: int, municipio_ibge: int) -> DataFrame:\n",
    "    df_base_pf = spark.range(num_pf).withColumn(\"id_natureza\", F.lit(1))\n",
    "    df_base_pj = spark.range(num_pj).withColumn(\"id_natureza\", F.lit(2))\n",
    "    df_base_clientes = df_base_pf.union(df_base_pj)\n",
    "    return (df_base_clientes.withColumn(\"id\", F.expr(\"uuid()\"))\n",
    "        .withColumn(\"detalhes\", _gerar_detalhes_cliente_udf(F.col(\"id_natureza\")))\n",
    "        .select(\"id\", F.col(\"detalhes.nome\").alias(\"nome\"), \"id_natureza\",\n",
    "                F.col(\"detalhes.registro_nacional\").alias(\"registro_nacional\"),\n",
    "                F.col(\"detalhes.nascido_em\").alias(\"nascido_em\"),\n",
    "                F.lit(estado_ibge).alias(\"estado_ibge\"), F.lit(municipio_ibge).alias(\"municipio_ibge\")))\n",
    "\n",
    "# UDF Python que aceita 'seed'\n",
    "def _gerar_detalhes_conta_python(iterator: Iterator[pd.DataFrame], config: dict, tipos_conta: list, ispbs: list, ano_estatistica: int, seed: int) -> Iterator[pd.DataFrame]:\n",
    "    # Garantir determinismo dentro da UDF\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    local_fake = Faker('pt_BR')\n",
    "    Faker.seed(seed) # Semeia a instância local do Faker\n",
    "    \n",
    "    data_limite_abertura = datetime(ano_estatistica, 1, 1).date()\n",
    "    for lote in iterator:\n",
    "        resultados = []\n",
    "        for row in lote.itertuples(index=False):\n",
    "            is_high_risk = 1 if random.random() < config['PROB_CONTA_ALTO_RISCO'] else 0\n",
    "            if is_high_risk == 1:\n",
    "                start_date_relativa = timedelta(days=180)\n",
    "                data_inicio_recente = data_limite_abertura - start_date_relativa\n",
    "                aberta_em = local_fake.date_between(start_date=data_inicio_recente, end_date=data_limite_abertura)\n",
    "            else:\n",
    "                aberta_em = local_fake.date_between(start_date='-10y', end_date=data_limite_abertura)\n",
    "            if row.id_natureza == 1:\n",
    "                saldo = round(np.random.lognormal(mean=6, sigma=1.5), 2)\n",
    "                tipo_conta = random.choice(tipos_conta)\n",
    "            else:\n",
    "                saldo = round(np.random.lognormal(mean=9, sigma=1.8), 2)\n",
    "                tipo_conta = random.choice([c for c in tipos_conta if c in [1, 3]])\n",
    "            resultados.append({\"id\": str(uuid.uuid4()), \"saldo\": saldo, \"aberta_em\": aberta_em, \"agencia\": local_fake.numerify('####'), \n",
    "                               \"numero\": local_fake.numerify('#####-#'), \"id_tipo_conta\": tipo_conta, \"ispb_instituicao\": random.choice(ispbs),\n",
    "                               \"id_cliente\": row.id_cliente, \"is_high_risk\": is_high_risk})\n",
    "        yield pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_contas(df_clientes: DataFrame, estado_ibge: int, municipio_ibge: int, seed: int) -> DataFrame:\n",
    "    # Adicionada seed ao F.rand()\n",
    "    df_clientes_com_num_contas = df_clientes.withColumn(\"num_contas\", \n",
    "        F.when(F.col(\"id_natureza\") == 1, F.floor(F.rand(seed=seed) * 2) + 1)\n",
    "         .otherwise(F.floor(F.rand(seed=seed + 1) * 5) + 1))\n",
    "         \n",
    "    df_contas_base = df_clientes_com_num_contas.select(F.col(\"id\").alias(\"id_cliente\"), \"id_natureza\", F.explode(F.sequence(F.lit(1), F.col(\"num_contas\"))))\n",
    "    \n",
    "\n",
    "    gerador_com_contexto = functools.partial(_gerar_detalhes_conta_python, \n",
    "                                             config=config_geracao, \n",
    "                                             tipos_conta=LISTA_TIPOS_CONTA_LOCAL, \n",
    "                                             ispbs=LISTA_ISPBS_LOCAL, \n",
    "                                             ano_estatistica=ANO_ESTATISTICA,\n",
    "                                             seed=seed) # <-- Seed injetada\n",
    "                                             \n",
    "    return (df_contas_base.mapInPandas(gerador_com_contexto, schema=SCHEMA_CONTAS_UDF)\n",
    "            .withColumn(\"estado_ibge\", F.lit(estado_ibge)).withColumn(\"municipio_ibge\", F.lit(municipio_ibge)))\n",
    "\n",
    "def _gerar_detalhes_chave_udf(iterator: Iterator[pd.DataFrame], config: dict, seed: int) -> Iterator[pd.DataFrame]:\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    local_fake = Faker('pt_BR')\n",
    "    Faker.seed(seed) \n",
    "\n",
    "    for lote in iterator:\n",
    "        resultados = []\n",
    "        for row in lote.itertuples(index=False):\n",
    "            try:\n",
    "                data_abertura_obj = pd.to_datetime(row.aberta_em).date()\n",
    "                dias_para_cadastrar = random.randint(1, config['MAX_DIAS_CADASTRO_CHAVE_RISCO']) if hasattr(row, 'is_high_risk') and row.is_high_risk == 1 else random.randint(1, 90)\n",
    "                cadastrada_em = data_abertura_obj + timedelta(days=dias_para_cadastrar)\n",
    "                if row.id_natureza == 1:\n",
    "                    tipos_possiveis = {1: row.registro_nacional, 2: local_fake.email(), 3: local_fake.phone_number(), 4: str(uuid.uuid4())}\n",
    "                else:\n",
    "                    tipos_possiveis = {5: row.registro_nacional, 2: local_fake.company_email(), 4: str(uuid.uuid4())}\n",
    "                tipo_chave = random.choice(list(tipos_possiveis.keys()))\n",
    "                resultados.append({\"id\": str(uuid.uuid4()), \"chave\": tipos_possiveis[tipo_chave], \"id_tipo_chave\": tipo_chave, \"cadastrada_em\": cadastrada_em, \"id_conta\": row.id_conta})\n",
    "            except Exception as e: \n",
    "                sys.stderr.write(f\"ERRO NA GERAÇÃO DE CHAVES: {e}, DADOS: {row}\\n\")\n",
    "                raise e\n",
    "        if resultados: yield pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_chaves_pix(df_contas_completo: DataFrame, df_clientes_completo: DataFrame, estado_ibge: int, municipio_ibge: int, seed: int) -> DataFrame:\n",
    "    df_contas_com_cliente = df_contas_completo.join(df_clientes_completo, df_contas_completo.id_cliente == df_clientes_completo.id, \"inner\").select(df_contas_completo.id.alias(\"id_conta\"), \"id_natureza\", \"registro_nacional\", \"aberta_em\", df_contas_completo.is_high_risk)\n",
    "    \n",
    "    # Passando a seed para a UDF\n",
    "    gerador_com_contexto = functools.partial(_gerar_detalhes_chave_udf, \n",
    "                                             config=config_geracao,\n",
    "                                             seed=seed) # <-- Seed injetada\n",
    "                                             \n",
    "    return (df_contas_com_cliente.mapInPandas(gerador_com_contexto, schema=SCHEMA_CHAVES_UDF)\n",
    "            .withColumn(\"estado_ibge\", F.lit(estado_ibge)).withColumn(\"municipio_ibge\", F.lit(municipio_ibge)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eeecfa0-d4cf-4119-b18f-0740243de0fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funções de Geração de Eventos (Transações) e sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8832b48a-7604-4627-bb5f-88a5dba58cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _obter_params_tempo(ano: int, mes: int) -> tuple:\n",
    "    num_dias = calendar.monthrange(ano, mes)[1]\n",
    "    primeiro_dia = datetime(ano, mes, 1)\n",
    "    ultimo_dia = datetime(ano, mes, num_dias, 23, 59, 59)\n",
    "    return primeiro_dia, (ultimo_dia - primeiro_dia).total_seconds()\n",
    "\n",
    "# Função interna agora usa 'random_instance'\n",
    "def _aplicar_horario_suspeito(data_transacao, config, random_instance):\n",
    "    if random_instance.random() < config.get('PROBABILIDADE_ATAQUE_MADRUGADA', 0.70):\n",
    "        return data_transacao.replace(hour=random_instance.randint(1, 4), minute=random_instance.randint(0, 59))\n",
    "    return data_transacao\n",
    "\n",
    "def _obter_perfil_sazonal_mes(ano: int, mes: int) -> dict:\n",
    "    pesos_dia_semana = { 0: 1.2, 1: 1.1, 2: 1.1, 3: 1.1, 4: 1.3, 5: 0.8, 6: 0.7 }\n",
    "    pesos_comemorativos = { (1, 1): 0.5, (5, 10): 2.0, (6, 12): 1.8, (8, 11): 1.8, (10, 12): 1.5, (11, 25): 3.0, (12, 20): 2.0, (12, 23): 2.5, (12, 24): 3.0, (12, 25): 1.0, (12, 31): 1.5 }\n",
    "    num_dias = calendar.monthrange(ano, mes)[1]\n",
    "    perfil_final = {}\n",
    "    for dia in range(1, num_dias + 1):\n",
    "        data = datetime(ano, mes, dia)\n",
    "        dia_semana = data.weekday()\n",
    "        peso = pesos_dia_semana.get(dia_semana, 1.0)\n",
    "        if (mes, dia) in pesos_comemorativos: peso = pesos_comemorativos[(mes, dia)]\n",
    "        perfil_final[dia] = peso\n",
    "    return perfil_final\n",
    "\n",
    "def _gerar_detalhes_transacao_python_vetorizado(\n",
    "    iterator: Iterator[pd.DataFrame], ano: int, mes: int, \n",
    "    config: dict, perfis_uso: list, \n",
    "    perfil_sazonal: dict, seed: int \n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "\n",
    "    import random\n",
    "    import uuid\n",
    "    from datetime import timedelta\n",
    "    from itertools import chain\n",
    "    primeiro_dia, _ = _obter_params_tempo(ano, mes) \n",
    "    colunas_finais_transacoes = [\"id\", \"valor\", \"data\", \"mensagem\", \"id_conta_origem\", \"id_conta_destino\", \"id_tipo_iniciacao_pix\", \"id_finalidade_pix\", \"is_fraud\", \"fraud_type\", \"id_transacao_cadeia_pai\"]\n",
    "    PROB_RUIDO = 0.25; MENSAGENS_RUIDO = [\"Pagamento de Boleto\", \"Lanchonete\", \"Uber\", \"Recarga de Celular\"]\n",
    "    \n",
    "\n",
    "    try:\n",
    "        primeiro_lote = next(iterator)\n",
    "    except StopIteration:\n",
    "\n",
    "        return\n",
    "        \n",
    "    partition_id = int(primeiro_lote['pid'].iloc[0])\n",
    "    lote_seed = int(seed) + partition_id \n",
    "    \n",
    "\n",
    "    local_random = random.Random(lote_seed)\n",
    "    local_np_random = np.random.RandomState(lote_seed)\n",
    "    \n",
    "   \n",
    "    \n",
    "    for lote in chain([primeiro_lote], iterator):\n",
    "        n = len(lote)\n",
    "        if n == 0: continue\n",
    "        \n",
    "        \n",
    "        pool_de_contas_reais = pd.concat([\n",
    "            lote['id_conta_origem'], \n",
    "            lote['id_conta_destino']\n",
    "        ]).unique().tolist()\n",
    "        \n",
    "        if not pool_de_contas_reais:\n",
    "            pool_de_contas_reais = [str(uuid.uuid4()) for _ in range(10)]\n",
    "        \n",
    "        \n",
    "        num_dias = calendar.monthrange(ano, mes)[1]; dias_do_mes = list(range(1, num_dias + 1))\n",
    "        pesos = [perfil_sazonal.get(dia, 1.0) for dia in dias_do_mes]\n",
    "        probabilidades = np.array(pesos) / np.sum(pesos)\n",
    "        \n",
    "       \n",
    "        dias_sorteados = local_np_random.choice(dias_do_mes, size=n, p=probabilidades)\n",
    "        segundos_no_dia = local_np_random.uniform(0, 86399, n)\n",
    "        \n",
    "        deltas_dias = pd.to_timedelta(dias_sorteados - 1, unit='D')\n",
    "        deltas_segundos = pd.to_timedelta(segundos_no_dia, unit='s')\n",
    "        lote['data'] = primeiro_dia + deltas_dias + deltas_segundos\n",
    "        \n",
    "    \n",
    "        lote['pagador_nascido_em'] = pd.to_datetime(lote['pagador_nascido_em'], errors='coerce')\n",
    "        lote['pagador_idade'] = (lote['data'] - lote['pagador_nascido_em']).dt.days / 365.25\n",
    "        lote['pagador_idade'] = lote['pagador_idade'].fillna(-1) \n",
    "        lote['chave_destino_cadastrada_em'] = pd.to_datetime(lote['chave_destino_cadastrada_em'])\n",
    "        delta_dias = (lote['data'].dt.date - lote['chave_destino_cadastrada_em'].dt.date).dt.days \n",
    "        lote['is_high_risk'] = lote['is_high_risk'].fillna(0).astype(int)\n",
    "        \n",
    "        idade_alvo = config.get('IDADE_MINIMA_ALVO_ENG_SOCIAL', 65)\n",
    "        prob_fraude_alvo = config.get('PROBABILIDADE_FRAUDE_ENG_SOCIAL_ALVO', 0.75)\n",
    "        \n",
    "        cond_idade_alvo = (lote['pagador_idade'] >= idade_alvo)\n",
    "        cond_conta_risco = (lote['is_high_risk'] == 1)\n",
    "        cond_chave_recente = ((delta_dias >= 0) & (delta_dias <= config['DIAS_CHAVE_CONSIDERADA_RECENTE']))\n",
    "        \n",
    "        prob_fraude_dinamica = np.select(\n",
    "            [cond_idade_alvo, cond_conta_risco, cond_chave_recente], \n",
    "            [prob_fraude_alvo, config['PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO'], config['PROBABILIDADE_FRAUDE_CHAVE_RECENTE']], \n",
    "            default=config['PROBABILIDADE_FRAUDE_BASE']\n",
    "        )\n",
    "        \n",
    "       \n",
    "        lote['is_fraud'] = (local_np_random.rand(n) < prob_fraude_dinamica).astype(int)\n",
    "        \n",
    "       \n",
    "        multiplicadores = np.select([lote['is_fraud'] == 1, (~(lote['is_fraud'] == 1)) & (local_np_random.rand(n) < 0.04)], [config['MULTIPLICADOR_MAGNITUDE_FRAUDE'], 2.5], default=1.0)\n",
    "        valores_calculados = np.maximum(0.01, local_np_random.lognormal(mean=np.log(150), sigma=0.8, size=n) * multiplicadores).round(2)\n",
    "        condicao_abaixo_radar = (lote['is_fraud'] == 1) & (local_np_random.rand(n) < config.get('PROBABILIDADE_ABAIXO_RADAR', 0.4))\n",
    "        \n",
    "        \n",
    "        lote['valor'] = np.where(condicao_abaixo_radar, local_np_random.choice(config.get('VALORES_LIMITE_RADAR', [999.90]), n), valores_calculados)\n",
    "        \n",
    "        \n",
    "        probs_tipo_fraude = config.get('PROBABILIDADES_TIPO_FRAUDE'); \n",
    "        tipos_normais = {k: v for k, v in probs_tipo_fraude.items() if k != 'engenharia_social'}\n",
    "        if sum(tipos_normais.values()) == 0: tipos_normais = {'valor_atipico': 1.0}\n",
    "        total_prob_normal = sum(tipos_normais.values())\n",
    "        tipos_normais_norm = {k: v / total_prob_normal for k, v in tipos_normais.items()}\n",
    "        \n",
    "        # Usa o gerador semeado da partição\n",
    "        tipos_fraude_aleatorios = local_np_random.choice(list(tipos_normais_norm.keys()), n, p=list(tipos_normais_norm.values()))\n",
    "        \n",
    "        lote['fraud_type'] = np.where(\n",
    "            (lote['is_fraud'] == 1) & (cond_idade_alvo), \n",
    "            \"engenharia_social\", \n",
    "            np.where(lote['is_fraud'] == 1, tipos_fraude_aleatorios, None) \n",
    "        )\n",
    "        \n",
    "        # --- PREPARAÇÃO FINAL DO LOTE (Determinístico)\n",
    "        lote['id'] = [str(uuid.uuid4()) for _ in range(n)]; lote['mensagem'] = \"Pagamento via Pix\"; \n",
    "        lote['id_tipo_iniciacao_pix'] = local_np_random.randint(1, 4, n); \n",
    "        lote['id_finalidade_pix'] = local_np_random.randint(1, 5, n); \n",
    "        lote['id_transacao_cadeia_pai'] = None\n",
    "        \n",
    "        # Remove a coluna 'pid' antes de finalizar\n",
    "        colunas_para_dropar = ['chave_destino_cadastrada_em', 'is_high_risk', 'id_tipo_conta_origem', 'id_tipo_conta_destino', 'pagador_nascido_em', 'pagador_idade', 'pid']\n",
    "        lote_final = lote.drop(columns=[col for col in colunas_para_dropar if col in lote.columns])\n",
    "        \n",
    "        # --- LÓGICA DE MICRO-LOTE ---\n",
    "        resultados_micro_lote = []\n",
    "        TAMANHO_MICRO_LOTE = 2000\n",
    "        \n",
    "        # --- LÓGICA DE GERAÇÃO DE CADEIA (FAN-OUT E FAN-IN) ---\n",
    "        # Usa o gerador semeado da partição (local_random)\n",
    "        for row in lote_final.itertuples(index=False):\n",
    "            \n",
    "            # --- LÓGICA DE FAN-OUT (Dispersão) ---\n",
    "            if hasattr(row, 'is_fraud') and row.is_fraud and row.fraud_type == \"triangulacao_conta_laranja\":\n",
    "                if local_random.random() < config.get('PROBABILIDADE_TESTE_CONTA', 0.3):\n",
    "                    resultados_micro_lote.append({\"id\": str(uuid.uuid4()), \"valor\": round(local_random.uniform(0.01, 1.00), 2), \"data\": row.data - timedelta(minutes=local_random.randint(1, 5)), \"mensagem\": \"Teste\", \"id_conta_origem\": row.id_conta_origem, \"id_conta_destino\": row.id_conta_destino, \"id_tipo_iniciacao_pix\": 1, \"id_finalidade_pix\": 1, \"is_fraud\": 0, \"fraud_type\": None, \"id_transacao_cadeia_pai\": None})\n",
    "                \n",
    "                row_dict = row._asdict(); \n",
    "                row_dict['data'] = _aplicar_horario_suspeito(row_dict['data'], config, local_random) # <-- Instância passada\n",
    "                \n",
    "                min_prof = config.get('FANOUT_MIN_PROFUNDIDADE', 2); max_prof = config.get('FANOUT_MAX_PROFUNDIDADE', 4)\n",
    "                profundidade_alvo = local_random.randint(min_prof, max_prof)\n",
    "                id_fraude_raiz = row_dict['id']; resultados_micro_lote.append(row_dict)\n",
    "                contas_nivel_anterior = {row_dict['id_conta_destino']: row_dict['valor']}; id_pai_nivel_anterior = {row_dict['id_conta_destino']: id_fraude_raiz}; data_nivel_anterior = {row_dict['id_conta_destino']: row_dict['data']}\n",
    "                \n",
    "                for nivel_atual in range(2, profundidade_alvo + 1):\n",
    "                    contas_proximo_nivel = {}; id_pai_proximo_nivel = {}; data_proximo_nivel = {}\n",
    "                    if not contas_nivel_anterior: break\n",
    "                    for conta_origem, valor_origem in contas_nivel_anterior.items():\n",
    "                        min_larg = config.get('FANOUT_MIN_LARGURA', 2); max_larg = config.get('FANOUT_MAX_LARGURA', 5)\n",
    "                        num_subs = local_random.randint(min_larg, max_larg)\n",
    "                        \n",
    "                        # Usa o gerador semeado da partição\n",
    "                        valores_divididos = local_np_random.dirichlet(np.ones(num_subs)) * valor_origem\n",
    "                        \n",
    "                        for k in range(num_subs):\n",
    "                            id_transacao_filha = str(uuid.uuid4())\n",
    "                            id_conta_destino_filha = local_random.choice(pool_de_contas_reais) \n",
    "                            segundos_offset = local_random.uniform(60 * (nivel_atual-1), 3600 * (nivel_atual-1))\n",
    "                            data_transacao_filha = data_nivel_anterior[conta_origem] + timedelta(seconds=segundos_offset); \n",
    "                            data_transacao_filha = _aplicar_horario_suspeito(data_transacao_filha, config, local_random) # <-- Instância passada\n",
    "                            \n",
    "                            transacao_filha = {\"id\": id_transacao_filha, \"valor\": round(max(0.01, valores_divididos[k]), 2), \"data\": data_transacao_filha, \"mensagem\": f\"Dispersão N{nivel_atual} (Parte {k+1}/{num_subs})\", \"id_conta_origem\": conta_origem, \"id_conta_destino\": id_conta_destino_filha, \"id_tipo_iniciacao_pix\": local_random.randint(1, 3), \"id_finalidade_pix\": local_random.randint(1, 4), \"is_fraud\": 1, \"fraud_type\": row.fraud_type, \"id_transacao_cadeia_pai\": id_pai_nivel_anterior[conta_origem]}\n",
    "                            resultados_micro_lote.append(transacao_filha)\n",
    "                            contas_proximo_nivel[id_conta_destino_filha] = transacao_filha[\"valor\"]; id_pai_proximo_nivel[id_conta_destino_filha] = id_transacao_filha; data_proximo_nivel[id_conta_destino_filha] = data_transacao_filha\n",
    "                            \n",
    "                            if local_random.random() < PROB_RUIDO:\n",
    "                                for _ in range(local_random.randint(1, 3)):\n",
    "                                    segundos_offset_ruido = local_random.uniform(10, 3600)\n",
    "                                    id_conta_destino_ruido = local_random.choice(pool_de_contas_reais) \n",
    "                                    resultados_micro_lote.append({\"id\": str(uuid.uuid4()), \"valor\": round(local_random.uniform(7.5, 75.0), 2), \"data\": data_transacao_filha + timedelta(seconds=segundos_offset_ruido), \"mensagem\": random.choice(MENSAGENS_RUIDO), \"id_conta_origem\": id_conta_destino_filha, \"id_conta_destino\": id_conta_destino_ruido, \"id_tipo_iniciacao_pix\": 1, \"id_finalidade_pix\": 1, \"is_fraud\": 0, \"fraud_type\": None, \"id_transacao_cadeia_pai\": None})\n",
    "                    contas_nivel_anterior = contas_proximo_nivel; id_pai_nivel_anterior = id_pai_proximo_nivel; data_nivel_anterior = data_proximo_nivel\n",
    "            \n",
    "            # --- LÓGICA DE FAN-IN (Consolidação) ---\n",
    "            elif hasattr(row, 'is_fraud') and row.is_fraud and row.fraud_type == \"consolidacao_fundos\":\n",
    "                row_dict = row._asdict()\n",
    "                row_dict['data'] = _aplicar_horario_suspeito(row_dict['data'], config, local_random) # <-- Instância passada\n",
    "                resultados_micro_lote.append(row_dict)\n",
    "                id_fraude_raiz = row_dict['id']; data_fraude_raiz = row_dict['data']\n",
    "                id_conta_destino_final = row_dict['id_conta_destino'] \n",
    "                \n",
    "                min_fontes = config.get('MIN_FONTES_CONSOLIDACAO', 3); max_fontes = config.get('MAX_FONTES_CONSOLIDACAO', 10); janela_seg = config.get('JANELA_SEG_CONSOLIDACAO', 3600)\n",
    "                num_fontes_extras = local_random.randint(min_fontes, max_fontes)\n",
    "                \n",
    "                for k in range(num_fontes_extras):\n",
    "                    id_transacao_filha = str(uuid.uuid4())\n",
    "                    id_conta_origem_filha = local_random.choice(pool_de_contas_reais) \n",
    "                    segundos_offset = local_random.uniform(1, janela_seg)\n",
    "                    data_transacao_filha = data_fraude_raiz + timedelta(seconds=segundos_offset)\n",
    "                    data_transacao_filha = _aplicar_horario_suspeito(data_transacao_filha, config, local_random) # <-- Instância passada\n",
    "                    \n",
    "                    # Usa o gerador semeado da partição\n",
    "                    valor_filha = np.maximum(0.01, local_np_random.lognormal(mean=np.log(150), sigma=0.8) * config.get('MULTIPLICADOR_MAGNITUDE_FRAUDE', 30)).round(2)\n",
    "                    \n",
    "                    transacao_filha = {\"id\": id_transacao_filha, \"valor\": valor_filha, \"data\": data_transacao_filha, \"mensagem\": f\"Consolidação Nível 1 (Fonte {k+1}/{num_fontes_extras})\", \"id_conta_origem\": id_conta_origem_filha, \"id_conta_destino\": id_conta_destino_final, \"id_tipo_iniciacao_pix\": local_random.randint(1, 3), \"id_finalidade_pix\": local_random.randint(1, 4), \"is_fraud\": 1, \"fraud_type\": row.fraud_type, \"id_transacao_cadeia_pai\": id_fraude_raiz}\n",
    "                    resultados_micro_lote.append(transacao_filha)\n",
    "            \n",
    "            else:\n",
    "                resultados_micro_lote.append(row._asdict())\n",
    "            \n",
    "            if len(resultados_micro_lote) >= TAMANHO_MICRO_LOTE:\n",
    "                df_final = pd.DataFrame(resultados_micro_lote)\n",
    "                yield df_final[colunas_finais_transacoes]\n",
    "                resultados_micro_lote = []\n",
    "                \n",
    "    if resultados_micro_lote: \n",
    "        df_final = pd.DataFrame(resultados_micro_lote)\n",
    "        yield df_final[colunas_finais_transacoes]\n",
    "\n",
    "def gerar_transacoes(\n",
    "    df_contas_local: DataFrame, \n",
    "    df_clientes_local: DataFrame, \n",
    "    df_chaves_recentes_local: DataFrame, num_contas_local: int,\n",
    "    volume_total: int, estado_ibge: int, municipio_ibge: int, ano: int, mes: int, \n",
    "    municipios_processados: list,\n",
    "    perfil_sazonal: dict,\n",
    "    seed: int # Esta é a 'base_seed' (ex: GLOBAL_SEED + i + mes)\n",
    ") -> DataFrame:\n",
    "    \n",
    "    # Garantir determinismo na escolha do município\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # --- LÓGICA DE GERAÇÃO DE PARES (Determinística e Otimizada) ---\n",
    "    volume_intermunicipal = int(volume_total * PROBABILIDADE_TRANSACAO_INTERMUNICIPAL)\n",
    "    volume_local = volume_total - volume_intermunicipal\n",
    "    df_pares_locais = spark.createDataFrame([], SCHEMA_PARES)\n",
    "    df_pares_intermunicipais = spark.createDataFrame([], SCHEMA_PARES)\n",
    "    \n",
    "    if volume_local > 0 and num_contas_local > 1:\n",
    "        N_BUCKETS = max(100, int(num_contas_local / 10000)) \n",
    "        \n",
    "        # Adicionada seed ao F.rand()\n",
    "        df_origens = df_contas_local.select(\n",
    "            F.col(\"id\").alias(\"id_conta_origem\"), \n",
    "            (F.rand(seed=seed) * N_BUCKETS).cast(\"int\").alias(\"join_key\")\n",
    "        )\n",
    "        df_destinos = df_contas_local.select(\n",
    "            F.col(\"id\").alias(\"id_conta_destino\"), \n",
    "            (F.rand(seed=seed + 1) * N_BUCKETS).cast(\"int\").alias(\"join_key\") # Seed diferente\n",
    "        )\n",
    "        df_pares_locais = df_origens.join(df_destinos, \"join_key\") \\\n",
    "                                  .filter(F.col(\"id_conta_origem\") != F.col(\"id_conta_destino\")) \\\n",
    "                                  .select(\"id_conta_origem\", \"id_conta_destino\") \\\n",
    "                                  .limit(volume_local) # Limit após o join é OK aqui\n",
    "    \n",
    "    outros_municipios = [m for m in municipios_processados if m != municipio_ibge]\n",
    "    \n",
    "    if volume_intermunicipal > 0 and outros_municipios:\n",
    "        municipio_alvo = random.choice(outros_municipios) # Determinístico (random.seed() acima)\n",
    "        \n",
    "\n",
    "        fraction_origem = min(1.0, (volume_intermunicipal * 1.2) / max(1, num_contas_local))\n",
    "        \n",
    "        contas_origem = df_contas_local.select(\"id\") \\\n",
    "            .sample(withReplacement=False, fraction=fraction_origem, seed=seed) \\\n",
    "            .limit(volume_intermunicipal) \\\n",
    "            .withColumnRenamed(\"id\", \"id_conta_origem\") \\\n",
    "            .withColumn(\"join_key\", F.monotonically_increasing_id())\n",
    "\n",
    "        contas_destino_externas = spark.table(\"transacoes_db.copper.contas\") \\\n",
    "            .filter(F.col(\"municipio_ibge\") == municipio_alvo) \\\n",
    "            .select(\"id\") \\\n",
    "            .sample(withReplacement=False, fraction=fraction_origem, seed=seed + 1) \\\n",
    "            .limit(volume_intermunicipal) \\\n",
    "            .withColumnRenamed(\"id\", \"id_conta_destino\") \\\n",
    "            .withColumn(\"join_key\", F.monotonically_increasing_id())\n",
    "            \n",
    "        df_pares_intermunicipais = contas_origem.join(contas_destino_externas, \"join_key\") \\\n",
    "                                              .select(\"id_conta_origem\", \"id_conta_destino\")\n",
    "    \n",
    "    df_pares_total = df_pares_locais.union(df_pares_intermunicipais)\n",
    "    if df_pares_total.isEmpty(): return spark.createDataFrame([], SCHEMA_TRANSACOES_FINAL)\n",
    "    -\n",
    "    df_pares_enriquecidos = (df_pares_total\n",
    "        .join(df_contas_local.alias(\"orig\"), df_pares_total.id_conta_origem == F.col(\"orig.id\"), \"inner\")\n",
    "        .join(df_clientes_local.alias(\"cli_orig\"), F.col(\"orig.id_cliente\") == F.col(\"cli_orig.id\"), \"inner\")\n",
    "        .join(df_contas_local.alias(\"dest\"), df_pares_total.id_conta_destino == F.col(\"dest.id\"), \"inner\")\n",
    "        .join(df_chaves_recentes_local.alias(\"chaves\"), df_pares_total.id_conta_destino == F.col(\"chaves.id_conta\"), \"left\")\n",
    "        .select(\"id_conta_origem\", \"id_conta_destino\", F.col(\"orig.id_tipo_conta\").alias(\"id_tipo_conta_origem\"),\n",
    "                F.col(\"dest.id_tipo_conta\").alias(\"id_tipo_conta_destino\"), F.col(\"chaves.chave_destino_cadastrada_em\"),\n",
    "                F.col(\"dest.is_high_risk\"),\n",
    "                F.col(\"cli_orig.nascido_em\").alias(\"pagador_nascido_em\")\n",
    "               ))\n",
    "\n",
    "    \n",
    "    df_pares_com_pid = df_pares_enriquecidos.withColumn(\"pid\", F.spark_partition_id())\n",
    "    \n",
    "\n",
    "    gerador_com_contexto = functools.partial(\n",
    "        _gerar_detalhes_transacao_python_vetorizado, \n",
    "        ano=ano, mes=mes, config=config_geracao, \n",
    "        perfis_uso=perfis_de_uso_dict,\n",
    "        perfil_sazonal=perfil_sazonal,\n",
    "        seed=seed \n",
    "    )\n",
    "    \n",
    "    df_transacoes_bruto = df_pares_com_pid.mapInPandas(gerador_com_contexto, schema=SCHEMA_TRANSACOES_UDF)\n",
    "    return df_transacoes_bruto.withColumn(\"estado_ibge\", F.lit(estado_ibge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6528c66a-a291-498d-a2b9-3601af91e39b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funções utilitárias e de Orquestração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786f3ae7-5e2f-429f-a51e-997d3f6539a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def salvar_dataframe_em_delta(df: DataFrame, nome_tabela_completo: str, modo: str = \"append\"):\n",
    "    if df is None or df.isEmpty():\n",
    "        print(f\"AVISO: DataFrame para a tabela '{nome_tabela_completo}' está vazio. Nenhuma ação tomada.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"INFO: Salvando dados na tabela Delta: {nome_tabela_completo} (modo: {modo})...\")\n",
    "        df.write.format(\"delta\").mode(modo).saveAsTable(nome_tabela_completo)\n",
    "        print(f\"✅ SUCESSO: Dados salvos em {nome_tabela_completo}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERRO ao salvar '{nome_tabela_completo}': {e}\")\n",
    "        raise e\n",
    "\n",
    "# Funções agora aceitam e passam a 'seed'\n",
    "def gerar_e_salvar_populacao(num_pf: int, num_pj: int, estado_ibge: int, municipio_ibge: int, seed: int) -> None:\n",
    "    print(f\"INFO: Gerando e materializando população para {municipio_ibge} (PF: {num_pf}, PJ: {num_pj})...\")\n",
    "    \n",
    "    # (Não precisa de seed, _gerar_clientes usa UDF semeada na Célula 1)\n",
    "    df_clientes_gerado = _gerar_clientes(num_pf, num_pj, estado_ibge, municipio_ibge)\n",
    "    salvar_dataframe_em_delta(df_clientes_gerado, \"transacoes_db.copper.clientes\", modo=\"append\")\n",
    "    \n",
    "    # Recarregar para garantir consistência\n",
    "    df_clientes_materializado = spark.table(\"transacoes_db.copper.clientes\").filter(F.col(\"municipio_ibge\") == municipio_ibge)\n",
    "    \n",
    "    # Passando a seed\n",
    "    df_contas_gerado = _gerar_contas(df_clientes_materializado, estado_ibge, municipio_ibge, seed=seed)\n",
    "    salvar_dataframe_em_delta(df_contas_gerado, \"transacoes_db.copper.contas\", modo=\"append\")\n",
    "    \n",
    "    df_contas_materializado = spark.table(\"transacoes_db.copper.contas\").filter(F.col(\"municipio_ibge\") == municipio_ibge)\n",
    "    \n",
    "    # Passando a seed\n",
    "    df_chaves_pix = _gerar_chaves_pix(df_contas_materializado, df_clientes_materializado, estado_ibge, municipio_ibge, seed=seed + 1)\n",
    "    salvar_dataframe_em_delta(df_chaves_pix, \"transacoes_db.copper.chaves_pix\", modo=\"append\")\n",
    "    \n",
    "    print(f\"INFO: População para o município {municipio_ibge} adicionada com sucesso.\")\n",
    "\n",
    "def limpar_tabelas_de_destino():\n",
    "    print(\"INFO: Apagando tabelas de destino para recriação...\")\n",
    "    for tabela in [\"clientes\", \"contas\", \"chaves_pix\", \"transacoes\"]:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS transacoes_db.copper.{tabela}\")\n",
    "    print(\"INFO: ✅ Limpeza concluída.\")\n",
    "\n",
    "def criar_tabelas_de_destino():\n",
    "    print(\"INFO: Criando tabelas de destino com os schemas corretos...\")\n",
    "    tabelas = {\n",
    "        \"transacoes_db.copper.clientes\": (SCHEMA_CLIENTES_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.contas\": (SCHEMA_CONTAS_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.chaves_pix\": (SCHEMA_CHAVES_PIX_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.transacoes\": (SCHEMA_TRANSACOES_FINAL, [\"estado_ibge\"])\n",
    "    }\n",
    "    for nome, (schema, part_cols) in tabelas.items():\n",
    "        spark.createDataFrame([], schema).write.format(\"delta\").partitionBy(part_cols).mode(\"overwrite\").saveAsTable(nome)\n",
    "        print(f\"INFO: Tabela '{nome}' criada com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb6257f-453a-45ce-a888-55f4be6064dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Orquestração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4311c855-863d-4608-a3fe-c707e29a7e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CÉLULA 3: ORQUESTRAÇÃO (REFATORADA E DETERMINÍSTICA)\n",
    "# =============================================================================\n",
    "try:\n",
    "    # 1. Preparação das Tabelas\n",
    "    limpar_tabelas_de_destino()\n",
    "    criar_tabelas_de_destino()\n",
    "    print(\"=============================================================================\")\n",
    "    print(f\"INFO: Iniciando processo de geração ANUAL (Ano: {ANO_ESTATISTICA})...\")\n",
    "    \n",
    "    # 2. Leitura e Seleção de Municípios\n",
    "    print(\"INFO: Lendo volumes anuais da tabela agregada...\")\n",
    "    df_volumes_anuais = spark.table(\"transacoes_db.pix_baseline_metricas.volumes_anuais_por_municipio\").filter(F.col(\"Ano\") == ANO_ESTATISTICA)\n",
    "    \n",
    "    print(\"INFO: Rankeando municípios para seleção...\")\n",
    "    # Rank é determinístico, não precisa de seed\n",
    "    df_ranks_anuais = df_volumes_anuais.withColumn(\"rank_pagador_anual\", F.rank().over(Window.orderBy(F.col(\"volume_pagador_anual\").desc())))\n",
    "    \n",
    "    municipios_a_processar_lista = df_ranks_anuais.orderBy(F.col(\"rank_pagador_anual\").asc()).limit(LIMITE_MUNICIPIOS_PROCESSADOS).collect()\n",
    "    total_municipios = len(municipios_a_processar_lista)\n",
    "    print(f\"INFO: {total_municipios} municípios selecionados para processar.\")\n",
    "    \n",
    "    id_municipios_selecionados = [row[\"cod_ibge_municipio\"] for row in municipios_a_processar_lista]\n",
    "\n",
    "    print(\"INFO: Coletando estatísticas mensais para os municípios selecionados...\")\n",
    "    # Coletar para o driver é uma boa otimização para < 100k linhas (Databricks Free)\n",
    "    stats_mensal_pd = (spark.table(\"transacoes_db.pix_baseline_metricas.relacao_pagadores_recebedores\")\n",
    "                          .filter((F.col(\"ano\") == ANO_ESTATISTICA) & (F.col(\"Municipio_Ibge\").isin(id_municipios_selecionados)))\n",
    "                          .select(\"Municipio_Ibge\", \"Mes\", \"total_tx_pf_pagador\", \"total_tx_pj_pagador\").toPandas())\n",
    "    stats_mensal_pd['total_tx_pagador'] = stats_mensal_pd['total_tx_pf_pagador'] + stats_mensal_pd['total_tx_pj_pagador']\n",
    "    print(\"INFO: Estatísticas mensais coletadas com sucesso.\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # PASSO 1: GERAR TODA A POPULAÇÃO PRIMEIRO\n",
    "    # =============================================================================\n",
    "    print(\"\\n--- PASSO 1: GERANDO POPULAÇÃO PARA TODOS OS MUNICÍPIOS ---\")\n",
    "    \n",
    "    municipios_ja_processados = [] \n",
    "\n",
    "    for i, municipio_row in enumerate(municipios_a_processar_lista):\n",
    "        codigo_municipio = municipio_row[\"cod_ibge_municipio\"]\n",
    "        codigo_estado = municipio_row[\"cod_ibge_estado\"]\n",
    "        nome_municipio = municipio_row[\"municipio_nome\"]\n",
    "        \n",
    "        print(f\"\\nGerando População {i+1}/{total_municipios}: {nome_municipio} ({codigo_municipio})\")\n",
    "        \n",
    "        fator_escala_final = FATOR_ESCALA_VOLUME\n",
    "        volume_pf_anual = int(municipio_row[\"total_pf_anual\"] * fator_escala_final)\n",
    "        volume_pj_anual = int(municipio_row[\"total_pj_anual\"] * fator_escala_final)\n",
    "        num_pf = max(1, int(volume_pf_anual / (TX_POR_CLIENTE_ESPERADO * 12)))\n",
    "        num_pj = max(1, int(volume_pj_anual / (TX_POR_CLIENTE_ESPERADO * 12)))\n",
    "\n",
    "        # ! REATORAÇÃO: Passando seed única por município\n",
    "        gerar_e_salvar_populacao(\n",
    "            num_pf=num_pf, num_pj=num_pj, \n",
    "            estado_ibge=codigo_estado, municipio_ibge=codigo_municipio,\n",
    "            seed=GLOBAL_SEED + i # Seed única\n",
    "        )\n",
    "        municipios_ja_processados.append(codigo_municipio)\n",
    "\n",
    "    print(\"\\n--- PASSO 1 CONCLUÍDO: Todas as populações foram salvas. ---\")\n",
    "  \n",
    "    print(\"\\n--- PASSO 2 (REMOVIDO): Pool de contas será gerado dentro da UDF. ---\")\n",
    "\n",
    "    # =============================================================================\n",
    "    # PASSO 3: GERAR TRANSAÇÕES (SEM CACHE - Otimizado)\n",
    "    # =============================================================================\n",
    "    print(\"\\n--- PASSO 3: GERANDO TRANSAÇÕES MÊS A MÊS ---\")\n",
    "\n",
    "    for i, municipio_row in enumerate(municipios_a_processar_lista):\n",
    "        codigo_municipio = municipio_row[\"cod_ibge_municipio\"]\n",
    "        codigo_estado = municipio_row[\"cod_ibge_estado\"]\n",
    "        nome_municipio = municipio_row[\"municipio_nome\"]\n",
    "        \n",
    "        print(f\"\\n================== Iniciando Transações {i+1}/{total_municipios}: {nome_municipio} ({codigo_municipio}) ==================\")\n",
    "        \n",
    "        fator_escala_final = FATOR_ESCALA_VOLUME\n",
    "\n",
    "        # Esta é a leitura otimizada (fora do loop mensal)\n",
    "        print(\"INFO: Lendo dados do município para o loop mensal (sem cache)...\")\n",
    "        df_contas_do_municipio = spark.table(\"transacoes_db.copper.contas\") \\\n",
    "                                      .filter(F.col(\"municipio_ibge\") == codigo_municipio)\n",
    "        \n",
    "        df_clientes_do_municipio = spark.table(\"transacoes_db.copper.clientes\") \\\n",
    "                                        .filter(F.col(\"municipio_ibge\") == codigo_municipio)\n",
    "        \n",
    "        num_contas_do_municipio = df_contas_do_municipio.count()\n",
    "        if num_contas_do_municipio == 0:\n",
    "            print(f\"AVISO: Nenhuma conta encontrada para {nome_municipio}. Pulando...\")\n",
    "            continue\n",
    "        \n",
    "        df_chaves_do_municipio = spark.table(\"transacoes_db.copper.chaves_pix\") \\\n",
    "                                       .filter(F.col(\"municipio_ibge\") == codigo_municipio)\n",
    "        \n",
    "        window_chaves = Window.partitionBy(\"id_conta\").orderBy(F.col(\"cadastrada_em\").desc())\n",
    "        \n",
    "        df_chaves_recentes_do_municipio = (df_chaves_do_municipio\n",
    "                                            .withColumn(\"rank\", F.rank().over(window_chaves))\n",
    "                                            .filter(F.col(\"rank\") == 1)\n",
    "                                            .select(\"id_conta\", F.col(\"cadastrada_em\").alias(\"chave_destino_cadastrada_em\")))\n",
    "\n",
    "        for mes in range(1, 13):\n",
    "            print(f\"\\n--- Processando Mês {mes}/{ANO_ESTATISTICA} para {nome_municipio} ---\")\n",
    "            \n",
    "            stats_mensal = stats_mensal_pd[(stats_mensal_pd['Municipio_Ibge'] == codigo_municipio) & (stats_mensal_pd['Mes'] == mes)]\n",
    "            if stats_mensal.empty: \n",
    "                print(f\"AVISO: Sem estatísticas para {mes}/{ANO_ESTATISTICA}. Pulando.\"); continue\n",
    "            \n",
    "            volume_total_original = stats_mensal[\"total_tx_pagador\"].iloc[0]\n",
    "            volume_total = int(volume_total_original * fator_escala_final)\n",
    "            if volume_total <= 0: \n",
    "                print(f\"AVISO: Volume de transações base para {mes}/{ANO_ESTATISTICA} é 0. Pulando.\"); continue\n",
    "                \n",
    "            print(f\"       Volume Original: {volume_total_original} | Volume BASE Alvo: {volume_total}\")\n",
    "            \n",
    "            print(f\"        INFO: Gerando perfil de sazonalidade para o Mês {mes}...\")\n",
    "            perfil_sazonal_mes = _obter_perfil_sazonal_mes(ANO_ESTATISTICA, mes)\n",
    "            \n",
    "            # ! REATORAÇÃO: Passando seed única (município + mês)\n",
    "            df_transacoes = gerar_transacoes(\n",
    "                df_contas_local=df_contas_do_municipio,\n",
    "                df_clientes_local=df_clientes_do_municipio,\n",
    "                df_chaves_recentes_local=df_chaves_recentes_do_municipio,\n",
    "                num_contas_local=num_contas_do_municipio,\n",
    "                volume_total=volume_total, \n",
    "                estado_ibge=codigo_estado, \n",
    "                municipio_ibge=codigo_municipio,\n",
    "                ano=ANO_ESTATISTICA, \n",
    "                mes=mes, \n",
    "                municipios_processados=municipios_ja_processados,\n",
    "                perfil_sazonal=perfil_sazonal_mes,\n",
    "                seed=GLOBAL_SEED + i + mes # Seed única por task\n",
    "            )\n",
    "            salvar_dataframe_em_delta(df_transacoes, \"transacoes_db.copper.transacoes\", modo=\"append\")\n",
    "        \n",
    "        print(f\"INFO: Processamento do município {nome_municipio} concluído.\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\nINFO: O script chegou ao fim.\")\n",
    "\n",
    "print(\"\\n=============================================================================\")\n",
    "print(\"INFO: Processo de geração de dados sintéticos (determinístico) concluído.\")\n",
    "print(\"=============================================================================\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4900604027270253,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "_Gerador_de_Dados_Sinteticos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
